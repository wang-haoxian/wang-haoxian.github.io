[{"content":"An inevitable trend and parenting life As the rise of LLM and its application on coding field, more and more developers are trying to code with the help of AI. The most used are Anthropic\u0026rsquo;s Claude and Cursor, or similar tools. Some companies, for propaganda purpose, even claim that they are replace developers with AI. Despite the exaggeration, it\u0026rsquo;s really time to test them and embrace them to boost our productivity. Just yesterday, Mr. Karapathy tweeted about his own frustruation on catching up with the LLM trend, The original tweet. For us developers, it\u0026rsquo;s really a must to adapt to the new way of coding. Learning by using them it the best way. Meanwhile, with the kids, I cannot spend too much time on the computer. I need to find a way to vibe code on my phone, anywhere, anytime (waiting for the kids, on the bus, on the train, etc). So I started to explore how to make it possible.\nMy experience on vibe coding In my work, I have subscription of Cursor. I personnally subscribed to Github Copilot for a long time and tested both on VSCode and CLI. I also tested Gemini-cli, and breifly used Claude Code.\nRecently, Github proposed their Copilot Agent which is to use LLM agent to write code and work on Github. All you need is to tell it what you want in the repo and it will create a PR with the code to be reviewed by you. It\u0026rsquo;s really impressive. However I didn\u0026rsquo;t really enjoy it because I can\u0026rsquo;t deploy it to test without subscribe to Github Spark. Fine, I have to make my own vibe code environment.\nOpenAI made their effort to catch up with the trend. The recent released Codex-CLI got hyped and I found GPT-5 Codex to GPT-5.2 Codex are very promising models. I tested and I can\u0026rsquo;t be more satisfied. It\u0026rsquo;s really a nice tool to make relatively good quality code. After several days of test, I decided to make it available everywhere, especially on my iPhone. I started to explore how to integrate it into my mobile workflow.\nVibe code on iPhone with Codex-CLI The main idea is to use ssh to connect to a remote machine where Codex-CLI is installed. Then use a terminal app on iPhone to connect to it and vibe code everywhere. Simple as that. In reality, I want more than just typing and make it spit the code. I want to run the code and test the application. So a full development environment is needed. This is the workflow I came up with: Step 1: Prepare a remote machine It could be any machine that you can ssh into. It could be a cloud VM, a home server, your laptop with no sleep configuration or even an old Android phone with Termux installed. The only requirement is that it should have Codex-CLI installed and configured for ssh access. I just happened to have a bunch of Promox Machine lying around. I have cloud-init based VM templates ready to go. So I just created a new Ubuntu VM. With basic Python, NodeJS and Docker installed, it\u0026rsquo;s ready for development.\nThe additional step is to install Codex-CLI. You will need tmux or screen to keep the session alive when you disconnect from ssh.\nAlternatively, Docker is very good choice. Afterall, it\u0026rsquo;s strongly advised to isolate Codex-CLI from the host system. You can easily find tragedies on X or reddit that LLM Cli tools mess up someone\u0026rsquo;s system environment or even delete important data. For example: ⚠️ Warning: Gemini CLI Deleted My Entire Windows System. If you are bold enough, Codex-CLI or other LLM CLI toosl implemented much stronger sandboxing mechanism to avoid such tragedies. But who wants the hassle of recovering a messed up system?\nStep 2: Prepare iPhone terminal app There are several terminal apps on iOS that support ssh, or something else if you are on Android. I personnally use NeoServer: SSH Client|Terminal because I know the developper and I can get support from him. Other apps like Termius are also good choices. They are nothing more than the entrypoint for your workflow. Just install one of them and configure the ssh connection to your remote machine.\nThe specialty of NeoServer is that it supports launch or attach to a tmux session directly when connecting to the remote machine. This is very useful because you can keep your coding session alive even when you disconnect from ssh and with one click to go back to the workspace. At the same time, it has a special tmux-optimized command mode to make it easier to use tmux on iPhone. Step 3: prepare your coding environment The ubuntu VM is ready to use for most of the coding tasks, you just need to run npm i -g @openai/codex for Codex-CLI installation. Then run codex auth login to login to your OpenAI account and authorize Codex-CLI. Codex\u0026rsquo;s auth is kind of PITA, you need to ssh to your remote machine and use\n1 ssh -N -L 127.0.0.1:1455:127.0.0.1:1455 user@remote_machine to forward the port for Codex-CLI to auth. Luckily it\u0026rsquo;s oneshot, you just need to do it once. After that, you can use Codex-CLI normally. Checkout this issue for more details: Support remote / headless OAuth sign-in #2798\nI personally use linuxbrew to manage my packages. It\u0026rsquo;s very convenient to install different versions of NodeJS, Python, etc. You can also use nvm, pyenv or other version managers. Just make sure you have the right environment for your coding tasks.\nAdditionally, I suggest to get docker ready so that you can test your code in isolated containers in a detached manner with docker-compose. Since the Github Spark is paid only, this alternative is very useful to test web apps or other services while vibe coding.\nStep 4: Setup Codex-CLI Before using Codex-CLI, you need to setup your coding preferences. You can create a config file at ~/.codex/config.toml. I would like to share how I euip it. Firstly, it\u0026rsquo;s useful to allow codex to search for issues/docs on the web.\n1 2 3 4 5 [sandbox_workspace_write] network_access = true [features] web_search_request = true You can find more options of security here Docs for security. I didn\u0026rsquo;t touch too much to keep a balance between security and usability.\nMCPs are very useful these days, but we don\u0026rsquo;t need too many of them. I just enable the most useful ones for me. Install MCPs for Codex CLI\n1 2 3 4 5 6 7 8 [mcp_servers.chrome-devtools] command = \u0026#34;npx\u0026#34; args = [\u0026#34;chrome-devtools-mcp@latest\u0026#34;, \u0026#34;--headless=true\u0026#34;, \u0026#34;--isolated\u0026#34;] enabled = true [mcp_servers.context7] command = \u0026#34;npx\u0026#34; args = [\u0026#34;-y\u0026#34;, \u0026#34;@upstash/context7-mcp\u0026#34;] Where chrome-devtools is very useful for web development especially for smoke testing the web apps, and context7 is a general purpose MCP that can access docs for libraries. You can add more MCPs as you need. Just make sure to read their documentation for proper configuration.\nStep 5: get yourself notified when Agent Run is done When you are vibe coding, you may not want to stare at the terminal waiting for Codex-CLI to finish the task. If you are on desktop, the terminal can fire notification when the task is done. On iPhone, it\u0026rsquo;s not that easy. I personnally use ntfy service to send notification to my phone and it works with simple topic subscription. Just add this to your ~/.codex/config.toml file:\n1 2 3 4 5 notify = [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;curl -s -o /dev/null -d \\\u0026#34;Agent Turn Finished - $(date +\u0026#39;%H:%M\u0026#39;)\\\u0026#34; https://ntfy.sh/{your_topic}\u0026#34; ] Now when you run an Agent task, you will get notified on your phone when it\u0026rsquo;s done. Very convenient for vibe coding on the go. This command suppresses the curl output to avoid cluttering the terminal but simply tell you that it\u0026rsquo;s your turn.\nStep 6: Start vibe coding When everything is ready, you can start vibe coding on your iPhone. Just ssh into your remote machine with the terminal app, attach to your tmux session and start Codex-CLI. Don\u0026rsquo;t forget to setup your Agents.md file for your project. You can frame the behavior of the agent with proper instructions. For example, you can tell the agent to write code, test code, dockerize the app, etc. Just like you are working on your desktop machine.\nYou may also ask the agent to create Github Actions CI/CD workflows for your project. It\u0026rsquo;s very useful to automate your deployment tasks. Just tell the agent what you want and it will create the necessary files for you. With this, you can even leverage the Github Actions to build your docker images and deploy them to your server automatically as long as you have proper secrets setup in your repo.\nExtra step: Access the development server from your iPhone Not every family network gets public IP address. If you want to access your development server from outside, you may need to setup tailscale or Zerotier on your remote machine and iPhone. This way, you can access your development server securely from anywhere. Checkout Tailscale or Zerotier for more details.\nUI design Although vibe coding on terminal is good enough, sometimes a better UI is needed. The Codex-CLI suggested figma MCP but as a Muggle in design, I am not comfortable to use it. But Google just released Stitch+(https://stitch.withgoogle.com/) which is a AI assisted UI design tool. Or simply ask Gemini with screen captures to get better UI design. It\u0026rsquo;s well known that Gemini3 has strong capability in UI/UX design. You can leverage it to help you design better UI for your apps. Or you can run Gemini-cli to help you design UI components. Don\u0026rsquo;t hesitate to leverage different models for different tasks. Some even use one CLI to manage multiple CLIs to get the best of each model, but I haven\u0026rsquo;t tried that yet.\nWrap up I used this method for 1 month, and I am quite satisfied with the experience. I successfully made a complex stack with several Vue.js frontends, FastAPI services and PostgreSQL(in docker for dev environment for sure, since no one should use a simple DB inside docker for production). The whole stack is dockerized and can be deployed with docker-compose. It comes with tests and CI/CD workflows to automate the deployment with the capability to smoke test the web apps with chrome-devtools MCP. I didn\u0026rsquo;t squeeze every drop of productivity from vibe coding yet, but it\u0026rsquo;s already a great help for my coding tasks while taking care of my kids. And I find that the current workflow took already all my spare energy even though it\u0026rsquo;s not perfect yet.\n","date":"2025-12-27T13:00:06+09:00","permalink":"https://www.haoxian.icu/p/llm-vibe-coding-on-the-go-with-your-phone/","title":"[LLM] Vibe Coding on the go with your phone"},{"content":"Agentic Translation with validation In the previous post, I wrote about the LLM for translation. It was the most basic way to use the model to translate the text. However, the translation quality is not always good. In this post, I will try to use the model to generate the translation and validate it with a judge using SocietyOfMindAgent with AutoGen.\nAutogen Autogen is a framework for creating multi-agent AI applications that can act autonomously or work alongside humans creating by Microsoft. In this article, I will use the framework to create a translation service with a judge to validate the translation quality using the SocietyOfMindAgent.\nSocietyOfMindAgent Deepmind proposed in its paper Improving Factuality and Reasoning in Language Models through Multiagent Debate the \u0026ldquo;society of mind\u0026rdquo; approach inspired by Marvin Minsky\u0026rsquo;s theory of the same name. This approach is also called language generation through multi-agent debate. The society of mind is a collection of agents that work together to solve a problem. Each agent has its own expertise and can communicate with other agents to solve the problem.\nExample from Autogen\u0026rsquo;s documentation:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import asyncio from autogen_agentchat.ui import Console from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination async def main() -\u0026gt; None: model_client = OpenAIChatCompletionClient(model=\u0026#34;gpt-4o\u0026#34;) agent1 = AssistantAgent(\u0026#34;assistant1\u0026#34;, model_client=model_client, system_message=\u0026#34;You are a writer, write well.\u0026#34;) agent2 = AssistantAgent( \u0026#34;assistant2\u0026#34;, model_client=model_client, system_message=\u0026#34;You are an editor, provide critical feedback. Respond with \u0026#39;APPROVE\u0026#39; if the text addresses all feedbacks.\u0026#34;, ) inner_termination = TextMentionTermination(\u0026#34;APPROVE\u0026#34;) inner_team = RoundRobinGroupChat([agent1, agent2], termination_condition=inner_termination) society_of_mind_agent = SocietyOfMindAgent(\u0026#34;society_of_mind\u0026#34;, team=inner_team, model_client=model_client) agent3 = AssistantAgent( \u0026#34;assistant3\u0026#34;, model_client=model_client, system_message=\u0026#34;Translate the text to Spanish.\u0026#34; ) team = RoundRobinGroupChat([society_of_mind_agent, agent3], max_turns=2) stream = team.run_stream(task=\u0026#34;Write a short story with a surprising ending.\u0026#34;) await Console(stream) asyncio.run(main()) In this example, the society of mind agent holds two agents: assistant1(writer) and assistant2(editor). The two assistants are the basic text generation agents.\nA round robin group chat is used to switch between the two agents. A termination message is added to the group chat to stop the conversation when the editor approves the text. If there is no termination message defined, the conversation will continue indefinitely. This group chat is what we call the team that used by SocietyOfMindAgent.\nThe society of mind agent is in fact an agent that process all the discussions generated by the group chat. And it comes with an instruction to wrap the discussion with a response prompt to generate a response based on the discussion.\nThe default instruction is:\n1 2 3 4 Earlier you were asked to fulfill a request. You and your team worked diligently to address that request. Here is a transcript of that conversation: {conversation}\u0026#34; The default response prompt is:\n1 Output a standalone response to the original request, without mentioning any of the intermediate discussion. Finally the the society of mind agent is added to the group chat as a single agent to generate the final response.\nTo summary, the society of mind is the representative of a group of agents to generate the final response.\nThe experiment My case is a lot simpler than the example above. I just need to translate a text and make it validate by a judge. It could be a multi-turn translate and refine iteration. If the judge approves the translation, the conversation stops and the society of mind agent will generate the final response which is the translation.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 import asyncio from autogen_agentchat.ui import Console from autogen_agentchat.agents import AssistantAgent, SocietyOfMindAgent from autogen_ext.models.openai import OpenAIChatCompletionClient from autogen_agentchat.teams import RoundRobinGroupChat from autogen_agentchat.conditions import TextMentionTermination async def main() -\u0026gt; None: model_client = OpenAIChatCompletionClient( model=\u0026#34;qwen2.5:7b\u0026#34;, api_key=\u0026#34;ff\u0026#34;, base_url=\u0026#34;http://192.168.88.251:11434/v1\u0026#34;, model_capabilities={ \u0026#34;vision\u0026#34;: False, \u0026#34;function_calling\u0026#34;: False, \u0026#34;json_output\u0026#34;: True, }, ) translator = AssistantAgent( \u0026#34;translator\u0026#34;, model_client=model_client, system_message=\u0026#34;You are a translator good at translating texts. You translate the text with faithfulness to the original text.\u0026#34; ) translator_reviewer = AssistantAgent( \u0026#34;translator_reviewer\u0026#34;, model_client=model_client, system_message=\u0026#34;You are a reviewer good at reviewing translations. You review the translation for faithfulness, grammar and naming consistency. You provide feedback for the translation. Respond with \u0026#39;APPROVE\u0026#39; if the translation meets all the requirements.\u0026#34;) translation_termination = TextMentionTermination(\u0026#34;APPROVE\u0026#34;) translation_team = RoundRobinGroupChat( [translator, translator_reviewer], termination_condition=translation_termination, max_turns=3 ) response_prompt = \u0026#34;Output a standalone translation to the original request, without mentioning any of the intermediate discussion and the APPROVE message\u0026#34; society_of_mind_agent = SocietyOfMindAgent( \u0026#34;society_of_mind\u0026#34;, team=translation_team, model_client=model_client, response_prompt=response_prompt ) task = \u0026#34;\u0026#34;\u0026#34; Translate the following text to English: ### Si vous arrêtez d\u0026#39;utiliser REFRESH, collyre en récipient unidose ### \u0026#34;\u0026#34;\u0026#34; stream = society_of_mind_agent.run_stream(task=task) result = await Console(stream) return result.messages if __name__ == \u0026#34;__main__\u0026#34;: result = asyncio.run(main()) print(\u0026#34;-----------------\u0026#34;) print(result[-1].content) The above code is a simple translation service with a judge. The translation team holds two agents: translator and translator_reviewer. The conversation will stop when the reviewer approves the translation. The society of mind agent will generate the final response which is the translation.\nMy model is way smaller than the example. At the beginning, I used the Qwen2.5 1.5B model to translate the text. It works well most of the time. However, the translation is not consistent all the time and it will add some extra info that doesn\u0026rsquo;t exist in the original text. Much worse, the judge failed to spot the problem and approve the wrong translation. The 3B model failed to achieve the consistency and the judge process. At the end, I stopped the experiment at the 7B model. The model can generate the translation with high quality and the judge can spot the problem and approve the translation.\nHowever, it works as expected with minor changes. I changed the response prompt to only output the translation without mentioning the intermediate discussion and the APPROVE message. With the original response prompt, the final response contains the \u0026ldquo;APPROVE\u0026rdquo; message.\nThe final response is:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ---------- user ---------- Translate the following text to English: ### Si vous arrêtez d\u0026#39;utiliser REFRESH, collyre en récipient unidose ### ---------- translator ---------- If you stop using REFRESH, one-dose eye drops [Prompt tokens: 69, Completion tokens: 13] ---------- translator_reviewer ---------- APPROVE The provided translation is accurate and conveys the meaning of the original French text correctly. It uses appropriate English terminology for \u0026#34;REFRESH\u0026#34; (presumably a product name) and properly translates \u0026#34;collyre en récipient unidose\u0026#34; to \u0026#34;one-dose eye drops.\u0026#34; The sentence structure is clear and grammatically correct in English. [Prompt tokens: 105, Completion tokens: 74] ---------- society_of_mind ---------- If you stop using REFRESH, one-dose eye drops [Prompt tokens: 148, Completion tokens: 13] ---------- Summary ---------- Number of messages: 4 Finish reason: None Total prompt tokens: 322 Total completion tokens: 100 Duration: 2.78 seconds ----------------- If you stop using REFRESH, one-dose eye drops A second example:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 ---------- user ---------- Translate the following text to English: ### Faites attention avec DOLIPRANE 100 mg : · Si la douleur persiste plus de 5 jours, ou la fièvre plus de 3 jours, ou en cas d’efficacité insuffisante ou de survenue de tout autre signe, ne continuez pas le traitement sans l’avis de votre médecin. · La prise de paracétamol peut entraîner des troubles du fonctionnement du foie. · Vous devez demander l’avis de votre médecin avant de donner ce médicament à votre enfant : · s’il a une maladie du foie ou une maladie grave des reins, · s’il souffre de déshydratation, · s’il souffre par exemple de malnutrition chronique, s’il est en période de jeûne, s’il a perdu beaucoup de poids récemment, s’il est atteint du virus du SIDA ou d’une hépatite virale chronique, s’il souffre de mucoviscidose (maladie génétique et héréditaire caractérisée notamment par des infections respiratoires graves), ou encore s’il est atteint de la maladie de Gilbert (maladie héréditaire associée à une augmentation du taux de bilirubine dans le sang), · si votre enfant est allergique à l’aspirine et/ou aux anti-inflammatoires non stéroïdiens. · À titre informatif : la consommation de boissons alcoolisées pendant le traitement est déconseillée. En cas de sevrage récent d’un alcoolisme chronique, le risque d’atteinte hépatique est majoré · En cas d’administration chez un enfant, la dose dépend de son poids (voir rubrique « Comment utiliser DOLIPRANE 100 mg, suppositoire sécable ? »). ### ---------- translator ---------- Be careful when using DOLIPRANE 100 mg: - If pain persists for more than 5 days or fever for more than 3 days, or if the medication is not effective or other symptoms appear, do not continue treatment without your doctor\u0026#39;s advice. - Taking paracetamol can cause liver function problems. - Before giving this medicine to your child, consult your doctor: - If he has a liver disease or serious kidney illness, - If he suffers from dehydration, - For example, if he is chronically malnourished, fasting, has recently lost significant weight, has the HIV virus or chronic viral hepatitis, has mucoviscidosis (a genetic hereditary condition characterized by severe respiratory infections), or suffers from Gilbert\u0026#39;s syndrome (a hereditary disease associated with increased bilirubin levels in the blood), - If your child is allergic to aspirin and/or nonsteroidal anti-inflammatory drugs. - For information: Alcoholic beverages should be avoided during treatment. The risk of liver damage after chronic alcoholism withdrawal is increased. - When administering to a child, the dose depends on his weight (see \u0026#34;How to use DOLIPRANE 100 mg, cuttable suppository?\u0026#34; section). [Prompt tokens: 464, Completion tokens: 254] ---------- translator_reviewer ---------- APPROVE The translation accurately conveys all the information from the original text. It maintains the correct medical terminology and warning messages pertinent to using DOLIPRANE 100 mg responsibly. The sentence structure in English matches the intended meaning effectively while also ensuring that it is understandable to the target audience. Naming consistency has been preserved, and there are no grammatical errors noted within the provided translation. [Prompt tokens: 740, Completion tokens: 83] ---------- society_of_mind ---------- Be careful when using DOLIPRANE 100 mg: - If pain persists for more than 5 days or fever for more than 3 days, or if the medication is not effective or other symptoms appear, do not continue treatment without your doctor\u0026#39;s advice. - Taking paracetamol can cause liver function problems. - Before giving this medicine to your child, consult your doctor: - If he has a liver disease or serious kidney illness, - If he suffers from dehydration, - For example, if he is chronically malnourished, fasting, has recently lost significant weight, has the HIV virus or chronic viral hepatitis, has mucoviscidosis (a genetic hereditary condition characterized by severe respiratory infections), or suffers from Gilbert\u0026#39;s syndrome (a hereditary disease associated with increased bilirubin levels in the blood), - If your child is allergic to aspirin and/or nonsteroidal anti-inflammatory drugs. - For information: Alcoholic beverages should be avoided during treatment. The risk of liver damage after chronic alcoholism withdrawal is increased. - When administering to a child, the dose depends on his weight (see \u0026#34;How to use DOLIPRANE 100 mg, cuttable suppository?\u0026#34; section). [Prompt tokens: 397, Completion tokens: 254] ---------- Summary ---------- Number of messages: 4 Finish reason: None Total prompt tokens: 1601 Total completion tokens: 591 Duration: 19.30 seconds ----------------- Be careful when using DOLIPRANE 100 mg: - If pain persists for more than 5 days or fever for more than 3 days, or if the medication is not effective or other symptoms appear, do not continue treatment without your doctor\u0026#39;s advice. - Taking paracetamol can cause liver function problems. - Before giving this medicine to your child, consult your doctor: - If he has a liver disease or serious kidney illness, - If he suffers from dehydration, - For example, if he is chronically malnourished, fasting, has recently lost significant weight, has the HIV virus or chronic viral hepatitis, has mucoviscidosis (a genetic hereditary condition characterized by severe respiratory infections), or suffers from Gilbert\u0026#39;s syndrome (a hereditary disease associated with increased bilirubin levels in the blood), - If your child is allergic to aspirin and/or nonsteroidal anti-inflammatory drugs. - For information: Alcoholic beverages should be avoided during treatment. The risk of liver damage after chronic alcoholism withdrawal is increased. - When administering to a child, the dose depends on his weight (see \u0026#34;How to use DOLIPRANE 100 mg, cuttable suppository?\u0026#34; section). Final thoughts I had a lot of fun with this agent. In this very short exploration, I didn\u0026rsquo;t make sure the the society of mind at the end can generate a consistent response but only the inner team. The agent is somehow overkill for this use case. Most of the time, we can expect the model to output very good translation even with the 1.5B model. The judge can be more useful if the we provide much more context to the translation: for example, the domain of the text, the target audience, etc.\nThere are still a lot of things to explore with the agent. For example, I can add more agents to the society of mind agent to generate the final response. Or I can make the judge to vote instead of approve the translation to make the final translation more reliable.\nThis kind of generation by debate is very powerful. There are apps with a whole IT company built with agents this way like ChatDev. It is a very interesting approach to generate the final response. I will try to make a more complex agent in the future with other use cases.\nReferences Improving Factuality and Reasoning in Language Models through Multiagent Debate SocietyOfMindAgent ","date":"2025-02-01T11:00:06+09:00","permalink":"https://www.haoxian.icu/p/llm-society-of-mind-agent-for-translation/","title":"[LLM] Society of Mind Agent for translation"},{"content":"Introduction Whisper is a STT (Speech to Text) model developed by OPENAI. It\u0026rsquo;s a powerful model that can convert human speech into text. Friend of mine encoutered this project as an job interview task with IaC using Terraform so I get the idea to do it on my own and I find it interesting to deploy it on AWS Fargate. I chose Fargate because of the highly optimized version of it doesn\u0026rsquo;t require GPUs. In this post, I will share my journey to this final solution and show you how to deploy it.\nAll the code is available on Github and you can use it to deploy the model on your own AWS account.\nPrerequisites AWS CLI configured Terraform with Terraform Cloud (or local state if you prefer) configured Docker installed If you don\u0026rsquo;t have any experience on Terraform, you can use the official tutorial to get started: Getting Started with Terraform with AWS Provider.\nInterpretation of the task Due to the confidentiality of the task, I can\u0026rsquo;t share the original task. However, I can share the main points of the task. The task is to serve STT model to replace the previous outsourced service. The model is expected to serve 100 users in a call center for post-call analytics and we should expect X10 users in the future. My understanding on this task is:\nThis is not about the concurrency of the model. We don\u0026rsquo;t need real-time processing, so we can use batch processing. The number of users is not a significant factor for the service, but the number of audios generated and the processing time for each document. (For example, we want to have the transcriptions of all our last weeks calls to have insights in the weekly meetings this monday, that means the time window to process all the files before monday, so for the files of Friday we will have to process them during the weekend, without taking consideration of the time that analytics team should take). The model is not expected to be used by the public, so we don\u0026rsquo;t need to worry about the security of the model.(At least for this version, we can assume that we only use the model in a secure environment) The cost should be minimized. As long as we can optimize the model to use the least resources, we can use the cheapest service instead of the most powerful one with GPUs. The model should be scalable. We should be able to scale the model to serve more users in the future. We can create S3 buckets to store the audio files and the results, but we don\u0026rsquo;t have to since the end user of the service(the analytics Team) may have already had a solution for this. There are no limit on the choice of the model. We can use any model as long as it can serve the purpose. But we should be able to evaluate the model with some baseline metrics.(e.g. WER, CER, etc. But this is not the main point of the task) The Diarisation is not mentionned but important in the multi-party communications like telephones, but that should be another project since we are talking about another ML module. Reference of solutions on the market In France, I have encountered a few companies that provide STT services. I think it\u0026rsquo;s interesting to check out AlloMedia, a company that provides STT services for call centers. We can be inspired by their solutions.\nInvestigation on Whisper and its deployment Frameworks and Libraries There are several possible frameworks to use Whisper model, mainly:\nOpenAI official The official OpenAI implementation Whisper.CPP The C++ implementation of the model Transformers The Huggingface implementation of the model in its transformers library. Faster Whisper which converts the model to CTranslate2 format to optimize the model. There is a benchmark of these implementations on this page and I take some of the information here:\nLarge-v2 model on GPU Implementation Precision Beam size Time Max. GPU memory Max. CPU memory openai/whisper fp16 5 4m30s 11325MB 9439MB faster-whisper fp16 5 54s 4755MB 3244MB faster-whisper int8 5 59s 3091MB 3117MB Executed with CUDA 11.7.1 on a NVIDIA Tesla V100S.\nSmall model on CPU Implementation Precision Beam size Time Max. memory openai/whisper fp32 5 10m31s 3101MB whisper.cpp fp32 5 17m42s 1581MB whisper.cpp fp16 5 12m39s 873MB faster-whisper fp32 5 2m44s 1675MB faster-whisper int8 5 2m04s 995MB Executed with 8 threads on a Intel(R) Xeon(R) Gold 6226R.\nAs we can see, the faster-whisper implementation is the most optimized one. It\u0026rsquo;s interesting to use this implementation for our deployment.\nThis is why I chose to use CPU and Fargate. After some tests, I found that the base model is very optimized and can be used on CPU and we can get the result in a reasonable time without the cost of quality.\nThe code to serve the model is very simple, we can use the following code to serve the model:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 from faster_whisper import WhisperModel model_size = \u0026#34;large-v3\u0026#34; # Run on GPU with FP16 model = WhisperModel(model_size, device=\u0026#34;cuda\u0026#34;, compute_type=\u0026#34;float16\u0026#34;) # or run on GPU with INT8 # model = WhisperModel(model_size, device=\u0026#34;cuda\u0026#34;, compute_type=\u0026#34;int8_float16\u0026#34;) # or run on CPU with INT8 # model = WhisperModel(model_size, device=\u0026#34;cpu\u0026#34;, compute_type=\u0026#34;int8\u0026#34;) segments, info = model.transcribe(\u0026#34;audio.mp3\u0026#34;, beam_size=5) print(\u0026#34;Detected language \u0026#39;%s\u0026#39; with probability %f\u0026#34; % (info.language, info.language_probability)) for segment in segments: print(\u0026#34;[%.2fs -\u0026gt; %.2fs] %s\u0026#34; % (segment.start, segment.end, segment.text)) As you can see, the code is very simple and we can use it to serve the model.\nPossible Solutions on AWS There are several possible way to deploy the model on AWS, mainly:\nEC2: We can deploy the model on EC2 and use the autoscaling group to scale the model. Lambda: We can deploy the model on Lambda and use the API Gateway to serve the model. Fargate: We can deploy the model on Fargate and use the ECS to serve the model. SageMaker: We can deploy the model on SageMaker and use the endpoint to serve the model. EKS: We can deploy the model on EKS and use the Kubernetes to serve the model. I chose to use Fargate because it\u0026rsquo;s the most optimized solution for our case. We don\u0026rsquo;t need GPUs and we don\u0026rsquo;t need to worry about the infrastructure. We can use the Fargate to serve the model and we can use the ECS to scale the model.\nLet\u0026rsquo;s still checkout the pros and cons of each solution:\nSolution Pros Cons EC2 Full control of the infrastructure Need to manage the infrastructure and expensive when using GPU Lambda Serverless Limited to 15 minutes and no GPU Fargate Serverless No GPU SageMaker Managed service Expensive EKS Full control of the infrastructure Need to manage the infrastructure Overview of the solution I draw the pipeline of the deployment as follows: We can expect three parts in the deployment:\nCodes The Python code for model serving code: The code to serve the model. The Terraform code for infrastructure: The code to deploy the infrastructure. The infrastructure part: The infrastructure of code and CI/CD pipeline. This part is rather static and normally in the organization, it\u0026rsquo;s managed by the DevOps team or the Infra team. And it\u0026rsquo;s not the main part of the task and not necessary to be done with AWS. It can be Github Actions, Gitlab CI, Jenkins, etc. The model serving part: The model is served by the Fargate and the ECS. We can use the ECS to scale the model. Python Implementation Development details Python Project management: Poetry\nModel Framework: CTranslate2 for faster-whisper\nModel size: Base\nAPI Framework: FastAPI with Swagger UI and OpenAPI support\nModel is downloaded when the web server is loaded (Not the best practice)\nAn endpoint /transcribe for transcription\nA health check /healthz is exposed for healthcheck ( for the sake of time, no model status check is implemented in this version) it’s just a hook to see if the server responds\nA dockerfile with docker-compose is created for building the image. And to launch it easily without typing long commands for dev purpose.\nThe docker compose file has limited CPU and memory to help with resources provisioning\n.gitignore and .dockerignore are added for not including unwanted files in docker building process or in git.\nThe repo coexists with terraform codes, in reality they should be either separated or as submodules.\nFormatting and linting:\nBlack Pylint MyPy isort Pydantic is used to make schemes for the API\nA makefile is created for handy commands for testing\nTerraform Implementation I have no previous experience on terraform except several runs with the official tutorials. I learnt during the project. To begin with, I took GitHub - kayvane1/terraform-aws-huggingface-deploy to get inspired. ChatGPT was used to help me understand quickly the syntax and make sample codes based on my needs, but mostly I use the official documentation.\nTerraform cloud In this project I use terraform cloud all the time to simulate an env for collaboration as team (shared secrets, states, etc) It could be S3 backend or whatever backend supported by Terraform\nDevelopment details Boostrap The project is modularized to four modules, instead of just a file.\nI consider this part as basic infrastructure that is used around the whole infrastructure. It should be easily tested one by one.\nthere are four modules:\nCode Commit Code Build ECR IAM (to be put on corresponding modules instead of being an independent module) App Deployment In contrary to the bootstrap part, I think this part is much more exclusive to the app. So I put every components in a single ecs module with alb.tf, main.tf, etc.\nResources Estimation Based on the local runs, I begin with\nTake 1 2 CPUs + 4GB\nIn fact only 65 of 2 CPU and 11% of RAM was used\nSo reduced to 1 CPU + 2GB but it’s not sufficient can caused the service to stop\nTake2 1 CPU + 2GB This cause the container to be killed often Final choice 2 CPU + 4 GB of RAM should be ok but we need further stress test ALB strategy I use ALB\u0026rsquo;s target group for autoscaling the service. The desired number was set to 3 and be able to scale to 0. The minimum health percent of app was set to 50 to trigger the scale in.\nThoughts on potential improvements Security https (even for internal communication - zero trust) + dedicated domain name (if exposed to the internet, if it’s called by enduser’s PC, not sure what the users look like) + with IP whitelist to improve security dev/stage/prod env separation Finer grain RBAC Scalability Automatic Target Weights (ATW) → weighted lb. Combine different strategies with metrics to make the scaling efficiently or Least outstanding requests stress test Over-Provisioning Fine-Tune threshold for scaling based on metrics Rate Limit in app Queuing Efficiency Improve configurability with more tf variables instead of hardcoded ones Cache for Code Build tag commit for code build and tag image (versioning of images), make two pipelines one for dev CI another for publish based on branch/tag system region - I picked randomly triggers Fargate deploy refresh should be triggered in pipeline once the prod build is done Code commit should trigger code pipeline automatically with push/merge to main systematically use tags Functionality Is Terraform cloud the standard usage of Doctolib? Maybe just with S3 backend better log/ monitoring EFS for models Viability health check container → the health check is oversimplified The model is downloaded from HFhub which is an external service, and if it fails, the service fails. So the model file should be persistent to S3 or add to the image. P-99 TM-99 Housekeeping aws_ecr_lifecycle_policy for clean up old images that are not in use Cost Agnostic cost Optim Fallback Use AWS TTS as a fallback plan In the end This is what I have done in this short-term project. I will continue to improve the project and I will be happy to hear your feedback.\n","date":"2024-02-01T11:00:06+09:00","permalink":"https://www.haoxian.icu/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/","title":"[MLOps] Deployment of Whisper on AWS Fargate using Terraform"},{"content":"What is Data Management in MLOps? Data management in MLOps is the process of managing data in the machine learning lifecycle. It includes data collection, data storage, data versioning, data quality, data monitoring, data lineage, data governance, and data security.\nIn this article, we will focus on data versioning, data storage, and data quality. I have checked several ways to manage data in MLOps, and I will share my experience with you. Please feel free to leave a comment if you have any questions or suggestions. This is not a data engineering article, so my focus is not on the data engineering part. I will try to keep it simple and easy to understand.\nWhy do we need to manage data in MLOps? There are saying that most of works of data scientists are data cleaning and data preprocessing.\nThis is true. In the machine learning lifecycle, data is the most important part. The quality of data will directly affect the performance of the model. As we what we say all the time: \u0026ldquo;Garbage in, garbage out.\u0026rdquo; Therefore, we need to manage data in MLOps.\nIt\u0026rsquo;s a big challenge to manage data in MLOps, because data is always changing. Imagine that you have a dataset collected from production, and you have trained a model with the dataset. After a while, the data in production has changed. You need to retrain the model with the new dataset. How can you manage the data and the model? For a simple model, you may have many different kind of processings. Take HTML as an example, you may have different processings like:\nremoving tags removing stop words removing punctuation removing numbers removing special characters Normalizing URLs removing emojis removing non-target langugae words removing non-ASCII characters removing non-printable characters For some of the processings, you can do it on the fly while training/inference, but some of them you need to do it before training since it\u0026rsquo;s not your part of pipeline. Or you will need to align with other downstream users of the same data source. For the sake of cost, you may want to do it once and for all but with possible different versions. Meanwhile, the pipeline is separated to several steps, and each step may have different requirements for the data. For example, the first step may need the raw data, and the second step may need the data after removing tags. When the dataset is big enough, it\u0026rsquo;s hard to manage the data. Thus we need to manage the data in MLOps.\nWhat are the requirements for data management in MLOps? I am not here to talk about using very advanced tools like Delta Lake or Apache Iceberg. They have some fancy features like time travel. But for a small team or a small project, they are too heavy and the cost may be too high. So I will focus on the principles of data management in MLOps by taking some ideas from data engineering. From my personal experiences, the requirements for data management in MLOps are:\nTraceability: We need to know where the data comes from, and how the data is processed. The metadata of the data should be managed. Reproducibility: We need to be able to reproduce the data with certain procedures in case we delete the data temporarily. Versioning: Data should be managed with versions, and we need to be able to access the data with different versions. Scalability: The ability to manage the data in a scalable way with business growth. Flexibility: The possibility of changing the processing of the data. Cost: The cost should be reasonable for the business. Performance: There are data requires cocurrent read/write, and we need to manage the data with performance. Simplicity: New members should be able to understand the data management and tools easily. Automation: The pipeline should be able to run automatically with as less as possible human intervention, just like a CI/CD pipeline. Collaboration: The management process should allow multiple people to work on the same data. Security: We need to define the scope of access control for different people. This is not an exhaustive list, but it\u0026rsquo;s a good start. I separate these requirements into three groups:\nTraceability, Reproducibility, Versioning Scalability, Flexibility, Cost, Performance Simplicity, Automation, Collaboration, Security Traceability, Reproducibility, Versioning These three requirements represents the most general ideas in people\u0026rsquo;s mind when talking about data management. I put them together because they are highly coupled. Traceability means we need to know where the data comes from, and how the data is processed. Reproducibility means we need to be able to reproduce the data with some given steps. Versioning means we need to be able to manage the versions of the data incrementally. These three requirements together means we are able to understand at each step when and how the data is processed, with what kind of upstream data source, and then we are able to reproduce each step with the information we have. What we need for these three requirements are basically a set of informations to describe the runs of the pipeline.\nScalability, Flexibility, Cost, Performance These four elements together force us to think about the most efficient way to manage the data. Sevceral questions we need to ask ourselves are:\nHow to manage the data when the data is big enough? How to manage the data with the changing business requirements? How to manage the data with a reasonable cost? We need to find out the solution to make a balance between these four elements. It\u0026rsquo;s easy to use a huge Google BigQuery to manage the data at a very scalable and performant way, at the price of potentially sacrificing the flexibility and it may be too expensive. It\u0026rsquo;s also easy to allocate unlimited size of bucket storage in AWS S3, but it may be too expensive and we may lose some performance. Some advanced modern data warehouses like Snowflake may be a good choice, but it add too much overhead for a small team or a small project.\nAll I want to say it\u0026rsquo;s that there are no silver bullet, and we need to find out the best solution for our own case. Sometimes git-lfs is our best friend, sometimes it\u0026rsquo;s not. Sometimes a simple S3 bucket will be nice. And in many cases DVC could be cool at some point.\nSimplicity, Automation, Collaboration, Security These four elements comes together when there is more than one person working on the same project.\nIf the data is difficult to access, it will be hard to collaborate. Collaboration comes with security concerns.\nAnd bad automation will make things more complicated. Automation could be the source of secret leaks. A good solution should be at the balance of these four elements. There are so many tools that can help with us. For example, Argo Workflow, Airflow, Dagster, etc. Basically, we need a centralized place to click and run the pipeline, and we need to be able to manage the access control without knowing every detail of the pipeline.\nData Design Pattern The Medallion Architecture There are many ways to manage data in MLOps. I will introduce the Medallion Architecture for it\u0026rsquo;s simplicity and flexibility. From this super cool article The Medallion Architecture from Databricks, we can quickly build an idea of how to manage data in MLOps. There are basically three layers in the Medallion Architecture:\nBronze: Raw data Silver: Cleaned and conformed data Gold: curated business-level data / Feature Store Bronze Layer The Bronze layer is the raw data layer. It\u0026rsquo;s the data that is collected from the data source. It has not been processed yet, it may contain some garbage data.Let\u0026rsquo;s say there are data in format like JSON, but it\u0026rsquo;s not well structured. You may see some missing fields, or some fields are not in the right format. It\u0026rsquo;s not impossible that there are so many repeated data. You normally cannot read it directly without some processing. Generally it\u0026rsquo;s so called unstructured or semi-structured data. It\u0026rsquo;s the data that is closest to the data source.\nSilver Layer The Silver layer is the cleaned and conformed data layer. It\u0026rsquo;s the data that has been processed and cleaned. It may not contain all the information for the final tasks. For example, if you want to build a dataset to fine-tune a LLM for insurance domain, this layer may contain the data that is anonymized. It may not contain the data that is not related to the insurance domain.\nGold Layer The Gold layer is the curated business-level data layer. It\u0026rsquo;s the data that is ready to use for the final tasks. A good example is the Feature Store. It\u0026rsquo;s the data that is ready to use for the training and inference. It\u0026rsquo;s the data that is ready to use for the final tasks. If you are building a Retrieval Augmented Generation (RAG) system, it could be the Vector DB that contains the vectors of the documents.\nFree to choose the data store With the Medallion Architecture, we can manage the data flexibly with necessary control without stuck to a single data storage. For example, the raw data can be stored in a S3 bucket with versioning, and the cleansed and bussiness data can be stored in a database like Postgres or Delta Lake. Each layer can be managed with different tools with different requirements, or if your organization has a centralized data platform, you can use the same tool to manage all the layers. It\u0026rsquo;s important to adopt the right tool for the right layer based on your business requirements.\nData Quality Data quality is much more than just data cleaning. It\u0026rsquo;s the process of ensuring the data is fit for the purpose. The data should be accurate, complete, consistent, and timely. However, in practice, it\u0026rsquo;s not easy to achieve all the requirements. In MlOps, we tend to begin with relatively low quality data, and then improve the quality of the data incrementally. For example, the classic human-in-the-loop approach is a good way to improve the quality of the data and align with the business requirements. There are also tools that can help us to improve the quality of the data. For example, Great Expectations is a good tool to help us to define the expectations of the data, and then validate the data with the expectations. It\u0026rsquo;s a good way to improve the quality of the data. There are also other tools like Apache Griffin, etc. But these are a little bit too heavy for a small team or a small project. Another kind of data quality tools is something like CleanLab. It\u0026rsquo;s a tool to help us to identify the mislabeled data by analyzing the outliars. The strong connection between data quality and model quality makes it a good tool to improve the quality of the data semi-automatically.\nData Versioning General tools for data versioning Data versioning is the process of managing the versions of the data. It\u0026rsquo;s important to keep track of the versions of the data, and be able to access the data with different versions, hence we can reproduce the data with the same version. There are many tools that can help us to manage the versions of the data. For example, DVC, git-lfs, Neptune, Pachyderm, lakeFS, etc.\nWhile DVC and git-lfs are more like a git extension, Neptune, Pachyderm, and lakeFS are more like a data lake. They are more like a centralized data platform. For a restrained budget, DVC and git-lfs are more suitable. For a big organization, tools like Neptune, Pachyderm, and lakeFS are good choices. And sometimes the infra team may be reluctant to add another tool to the stack, so we may need to use the existing tools like git-lfs.\nHome made solution Sometimes we don\u0026rsquo;t need to use any tools but well defined procedures. For example, we can use a S3 bucket to store the data, and use the folder structure to manage the versions of the data. It\u0026rsquo;s simple and easy to understand, it\u0026rsquo;s scalable with the business growth, and it\u0026rsquo;s cost effective. The only drawback is that it\u0026rsquo;s not easy to manage the access control in some fine-grained way. But it\u0026rsquo;s not a big problem for a small team or a small project. With a proper defined toolkit and workflow, it\u0026rsquo;s easy to manage the data with versioning this way.\nDescribe the data with metadata The dataset is often not obvious to understand. For instance, say you use parquets to store data, and you have a parquet file with 100 columns. It\u0026rsquo;s not easy to understand the data without any metadata. If you use a data warehouse like Snowflake or Delta Lake, you have easy access to the metadata of the data because they are built-in. But if you use a S3 bucket to store the data, you need to manage the metadata by yourself.\nIn the point of view of data engineering, the major metadata of the data is the schema of the data. In the point of view of data science, the major metadata of the data is the statistics of the data. From a business point of view, the metadata we want to see is a self-explanatory description of the data which matches the business logic. And in NLP, we may want to see the text normalization rules, the tokenization rules, etc. This is to say, the metadata is a complex set of information that varies from different perspectives and use cases. For me, we need to manage the metadata in a strucutred way, and we need to be able to access the metadata easily.\nAccessibility of the metadata The metadata should be positionned alongside the dataset. For databases, they are built-in. For more generic file systems like S3, we need to manage the metadata by ourselves. I would recommand formats that are easy to read and write, like JSON, YAML, or TOML, etc. This allows us to preview the metadata easily.\nThe reason to choose this kind of formats, is that we may want to use the metadata in the pipeline. For example, we may want to use the metadata to validate the data, or we may want to use the metadata to generate the documentation of the data. This requires the data to be in a structured format which can be easily parsed.\nSometimes an extract of the data is a good way to describe the data. It\u0026rsquo;s a good idea to put an sample of the data alongside the dataset. We may not want to download the whole dataset to build a first impression of the data. And we can use the sample to explain the whole dataset and the pipeline to non technical people.\nWhat to add to the metadata I have made a list of possible metadata that we may want to manage:\nDescription: What is the data about? Data source: What\u0026rsquo;s the upstream data source? SHA1: Useful to check data integrity Schema: Allows us to load the data easily Statistics: help us to handle the data (like missing values, etc.) Number of rows Number of columns Sample: A sample of the data if the data is compatible with the format, otherwise you may want it to be in a separate file ETL pipeline stage: What\u0026rsquo;s the current stage of the data in the pipeline? Exectuion time of the pipeline Mapping between the columns and the business logic Personally, there is always a rule of thumb to determine the complexity of something. That is, if someone new onboarding the project, how long will it take for him/her to understand the thing. The metadata here should be auto-representative not only for the data itself, but also for the pipeline or the code.\nExample of HTML data for NLP with the Medallion Architecture We have talked about HTML data in the beginning of this article. Let\u0026rsquo;s take a look at how we can manage the data with the Medallion Architecture. Let\u0026rsquo;s assume that you want to build a system to categorize the HTML documents. The data source is a crawler that crawls the HTML documents from the Internet, at a rate of 1000 documents per day. The output of the crawler is a JSON file that contains the HTML documents stored in a S3 bucket. In this example, we will limit the tools to show the case in a restrained budget. So we only use git and S3 with MLFlow. All workflows run on ArgoWorkflow or Airflow. Let\u0026rsquo;s name this project as html-classification and let\u0026rsquo;s begin.\nPipeline The pipeline will be essentially something like this: Architecture The architecture is illustrated as below: Overview of different layers The nomclature for the paths to the data in S3 We are going to setup a bunch of rules since we are using S3 to store the data. We will use the following nomclature for the paths to the data in S3:\n1 s3://\u0026lt;bucket_name\u0026gt;/html-classification/\u0026lt;stage\u0026gt;/\u0026lt;date-of-data\u0026gt;/\u0026lt;dataset\u0026gt; Bronze Layer At this stage, the most direct input is the HTML documents collected by the crawler. It can be the output of an ingestion workflow that streams the documents from some message queue system to process it in a batch way. The output of the ingestion workflow is a JSON file that contains the HTML documents. As a personal preference, I use JSONL format to facilitate the reading in streaming mode.\nThe format looks like:\n1 2 3 4 # s3://\u0026lt;bucket_name\u0026gt;/html-classification/raw/2021-09-01/html-documents.json {\u0026#34;url\u0026#34;: \u0026#34;https://www.example.com/giberish_html_?\u0026#34;,\u0026#34;html\u0026#34;: \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;...\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34;} {\u0026#34;url\u0026#34;: \u0026#34;https://www.example2.co/giberish_html_?\u0026#34;,\u0026#34;html\u0026#34;: \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;...\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34;} ... Metadata:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 - Description: The raw HTML documents collected by the crawler. Data source: The crawler Date of data creation: 2021-09-01 workflow-name: crawler-to-s3 SHA1: 8b8ae027744d2f920eb1aeee676d589957de40cc Schema: - url: string - html: string - timestamp: string Statistics: - Number of rows: 1000 - Number of columns: 3 Sample: - url: \u0026#34;https://www.example.com/giberish_html_?\u0026#34; html: \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;...\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34; timestamp: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34; - url: \u0026#34;https://www.example2.co/giberish_html_?\u0026#34; html: \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;...\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34; timestamp: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34; Silver Layer At this stage, we begin to have some much more structured data.\nURL Normalization The first step is to normalize the URLs, with some simple deduplication with title (This is a rather arbitrary process as example only. In real life, the dedup should be much more complex and taking account more information. For example, the same page with different timestamps. We tend to keep the fresh copy). The output of this step will be like:\n1 2 3 4 # s3://\u0026lt;bucket_name\u0026gt;/html-classification/preprocess/url-normalization/2021-09-02/html-documents.json {\u0026#34;url\u0026#34;: \u0026#34;https://www.example.com\u0026#34;,\u0026#34;html\u0026#34;: \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;...\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34;} {\u0026#34;url\u0026#34;: \u0026#34;https://www.example2.com\u0026#34;,\u0026#34;html\u0026#34;: \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;...\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34;} ... Metadata:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 - Description: The raw HTML documents collected by the crawler, with basic deduplication and normalization of the URLs. Data source: s3://\u0026lt;bucket_name\u0026gt;/html-classification/raw/2021-09-01/html-documents.json Date of data creation: 2021-09-02 workflow-name: ingestion SHA1: da4eec8e1ffe93df6b8a768ac78d98b3879a5baa Schema: - url: string - html: string - timestamp: string Statistics: - Number of rows: 998 # 2 duplicated rows removed - Number of columns: 3 Sample: - url: \u0026#34;https://www.example.com\u0026#34; html: \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;...\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34; timestamp: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34; - url: \u0026#34;https://www.example2.com\u0026#34; html: \u0026#34;\u0026lt;html\u0026gt;\u0026lt;body\u0026gt;...\u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt;\u0026#34; timestamp: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34; This will allow us to have unique URLs for the same document, and the url can be used as the primary key of the data.\nHTML Parsing This time the format will be like:\n1 2 3 4 # s3://\u0026lt;bucket_name\u0026gt;/html-classification/preprocess/html-parsing/2021-09-02/html-documents.json {\u0026#34;url\u0026#34;: \u0026#34;https://www.example.com\u0026#34;,\u0026#34;title\u0026#34;: \u0026#34;Example\u0026#34;,\u0026#34;content\u0026#34;: \u0026#34;This is an example.\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34;} {\u0026#34;url\u0026#34;: \u0026#34;https://www.example2.com\u0026#34;,\u0026#34;title\u0026#34;: \u0026#34;Example2\u0026#34;,\u0026#34;content\u0026#34;: \u0026#34;This is an example2.\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34;} ... Metadata:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 - Description: HTML documents with title and content extracted. Data source: s3://\u0026lt;bucket_name\u0026gt;/html-classification/preprocess/url-normalization/2021-09-02/html-documents.json Date of data creation: 2021-09-02 workflow-name: html-parsing SHA1: 8b8ae027744d2f920eb1aeee676d589957de40cc Schema: - url: string - title: string - content: string - timestamp: string Statistics: - Number of rows: 998 - Number of columns: 4 Sample: - url: \u0026#34;https://www.example.com\u0026#34; title: \u0026#34;Example\u0026#34; content: \u0026#34;This is an example.\u0026#34; timestamp: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34; - url: \u0026#34;https://www.example2.com\u0026#34; title: \u0026#34;Example2\u0026#34; content: \u0026#34;This is an example2.\u0026#34; timestamp: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34; This is just an illustration format. To be more realistic, we may want to have more information like the metadata of the page, language of the document, the encoding of the document, etc. But hey, we can use some NLP tools to process the data.\nText Normalization Now we want to apply some text normalization rules to the content. The choice of the rules depends on the business requirements. For example, we may want to remove the stop words, remove the punctuation, remove the numbers, remove the special characters, etc. And this could make huge difference for the final performance of the model. In this example, we could have several different versions of the data with different text normalization rules. And the metadata should come in handy to help us to understand the data.\nVersion 1 1 2 3 4 # s3://\u0026lt;bucket_name\u0026gt;/html-classification/preprocess/text-normalization/v1/2021-09-02/html-documents.json {\u0026#34;url\u0026#34;: \u0026#34;https://www.example.com\u0026#34;,\u0026#34;title\u0026#34;: \u0026#34;example\u0026#34;,\u0026#34;content\u0026#34;: \u0026#34;this is an example.\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34;} {\u0026#34;url\u0026#34;: \u0026#34;https://www.example2.com\u0026#34;,\u0026#34;title\u0026#34;: \u0026#34;example2\u0026#34;,\u0026#34;content\u0026#34;: \u0026#34;this is an example2.\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34;} ... Metadata:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 - Description: HTML documents with title and content extracted, and text normalization applied. Data source: s3://\u0026lt;bucket_name\u0026gt;/html-classification/preprocess/html-parsing/2021-09-02/html-documents.json Date of data creation: 2021-09-03 workflow-name: text-normalization-v1 process: - remove punctuation - remove special characters SHA1: 8b8ae027744d2f920eb1aeee676d589957de40cc Schema: - url: string - title: string - content: string - timestamp: string Statistics: - Number of rows: 998 - Number of columns: 4 Sample: - url: \u0026#34;https://www.example.com\u0026#34; title: \u0026#34;example\u0026#34; content: \u0026#34;this is an example\u0026#34; timestamp: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34; - url: \u0026#34;https://www.example2.com\u0026#34; title: \u0026#34;example2\u0026#34; content: \u0026#34;this is an example2\u0026#34; timestamp: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34; Version 2 1 2 3 4 # s3://\u0026lt;bucket_name\u0026gt;/html-classification/preprocess/text-normalization/v2/2021-09-02/html-documents.json {\u0026#34;url\u0026#34;: \u0026#34;https://www.example.com\u0026#34;,\u0026#34;title\u0026#34;: \u0026#34;example\u0026#34;,\u0026#34;content\u0026#34;: \u0026#34;this be an example.\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34;} {\u0026#34;url\u0026#34;: \u0026#34;https://www.example2.com\u0026#34;,\u0026#34;title\u0026#34;: \u0026#34;example2\u0026#34;,\u0026#34;content\u0026#34;: \u0026#34;this be an example2.\u0026#34;,\u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34;} ... Metadata:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 - Description: HTML documents with title and content extracted, and text normalization applied. Data source: s3://\u0026lt;bucket_name\u0026gt;/html-classification/preprocess/html-parsing/2021-09-02/html-documents.json Date of data creation: 2021-09-03 workflow-name: text-normalization-v2 process: - remove special characters - verbe lematization SHA1: 8b8ae027744d2f920eb1aeee676d589957de40cc Schema: - url: string - title: string - content: string - timestamp: string Statistics: - Number of rows: 998 - Number of columns: 4 Sample: - url: \u0026#34;https://www.example.com\u0026#34; title: \u0026#34;example\u0026#34; content: \u0026#34;this be an example.\u0026#34; timestamp: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34; - url: \u0026#34;https://www.example2.com\u0026#34; title: \u0026#34;example2\u0026#34; content: \u0026#34;this be an example2.\u0026#34; timestamp: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34; Now we have two versions of the data with different text normalization rules. We can use the metadata to understand the data. And we should be able to run model training with different versions of the data.\nGold Layer Now we need to have the data ready for our final task: document categorization. We need to have the data in a format that is ready to use for the training and inference.\nNow we have the basic text input, but we don\u0026rsquo;t have the targets for supervised learning. Let\u0026rsquo;s say we use an external paid API to get the targets. The output of the API is a JSON file that contains the targets.\n1 2 3 4 # s3://\u0026lt;bucket_name\u0026gt;/html-classification/annotated/2021-09-04/html-documents.json {\u0026#34;url\u0026#34;: \u0026#34;https://www.example.com\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;example - this be an example.\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;entertainment\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34;} {\u0026#34;url\u0026#34;: \u0026#34;https://www.example2.com\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;example2 - this be an example2.\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;politics\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34;} ... Metadata:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 - Description: HTML documents with title and content extracted, and text normalization applied, and targets added. Data source: - s3://\u0026lt;bucket_name\u0026gt;/html-classification/preprocess/text-normalization/v2/2021-09-03/html-documents.json # This allows us to trace the previous steps - https://www.example.com/api/v1/document-categorization # the api Date of data creation: 2021-09-04 workflow-name: annotation SHA1: 8b8ae027744d2f920eb1aeee676d589957de40cc Schema: - url: string - title: string - content: string - category: string - timestamp: string Statistics: - Number of rows: 998 - Number of columns: 5 Sample: - url: \u0026#34;https://www.example.com\u0026#34; title: \u0026#34;example\u0026#34; content: \u0026#34;this be an example.\u0026#34; category: \u0026#34;entertainment\u0026#34; timestamp: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34; - url: \u0026#34;https://www.example2.com\u0026#34; title: \u0026#34;example2\u0026#34; content: \u0026#34;this be an example2.\u0026#34; category: \u0026#34;politics\u0026#34; timestamp: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34; We use the v2 version of the data, and we add the targets to the data. Now we have the data ready for the training and inference.\nWhen we train the model, we can push the metadata to MLFlow, as artifact or as parameter. This allows us to trace the data and the model.\nPost Train Layer This is the step after the model training. We refine the data in this step with CleanLab. We try to rectify the mislabeled data and remove some outliers that are not related to the business.\n1 2 3 # s3://\u0026lt;bucket_name\u0026gt;/html-classification/post-train/2021-09-05/html-documents.json {\u0026#34;url\u0026#34;: \u0026#34;https://www.example.com\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;example - this be an example.\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;entertainment\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34;} {\u0026#34;url\u0026#34;: \u0026#34;https://www.example2.com\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;example2 - this be an example2.\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;politics\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34;} Metadata:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 - Description: HTML documents with title and content extracted, and text normalization applied, and targets added, and mislabeled data removed. Data source: - s3://\u0026lt;bucket_name\u0026gt;/html-classification/preprocess/annotated/2021-09-04/html-documents.json # This allows us to trace the previous steps Date of data creation: 2021-09-05 workflow-name: clean-lab SHA1: 8b8ae027744d2f920eb1aeee676d589957de40cc Schema: - url: string - title: string - content: string - category: string - timestamp: string Statistics: - Number of rows: 900 # reduced a lot of bad data - Number of columns: 5 Sample: - url: \u0026#34;https://www.example.com\u0026#34; title: \u0026#34;example\u0026#34; content: \u0026#34;this be an example.\u0026#34; category: \u0026#34;entertainment\u0026#34; timestamp: \u0026#34;2021-09-01T00:00:00+09:00\u0026#34; - url: \u0026#34;https://www.example2.com\u0026#34; title: \u0026#34;example2\u0026#34; content: \u0026#34;this be an example2.\u0026#34; category: \u0026#34;politics\u0026#34; timestamp: \u0026#34;2021-09-02T00:00:00+09:00\u0026#34; Conclusion This is a very simple data management example. But it shows the basic idea of how to manage data in MLOps.\nIt\u0026rsquo;s good to start with something simple and easy to understand. And then we can improve the data management incrementally.\nWe can begin with some naïve formats like JSONL, and then we can use more advanced formats like Parquet, Delta Table, etc. We can use some simple tools like plain S3FS or git-lfs, and then we can use more advanced tools like Neptune, Pachyderm, lakeFS, etc. We can use some simple workflow tools like Argo Workflow, and then we can use more advanced tools like Airflow, Dagster, etc.\nWe can also use MLFlow to track the data we used for the training.\nFor me, the most important thing is to be able to track the data the clean way to allow us to acheive the goals we mentioned in the beginning of this article. This example showed we can do it with some simple tools. Minio can help us control the access, the metadata can help us understand the data, and the workflow tools can help us to manage the pipeline. And all this with a restrained budget.\n","date":"2023-11-30T11:00:06+09:00","permalink":"https://www.haoxian.icu/p/mlops-data-management-in-mlops/","title":"[MLOps] Data Management in MLOps"},{"content":"Huggingface Pipeline as a Service I am currently working on some projects using Huggingface Transformers. I found it is a little bit tedious to set up a web server for each model. So I decided to write a script to automate the process.\nThere is a great documentation on Huggingface website about how to set up a web server for a model using Scarlette. Here is the link: pipeline_webserver\nQuick Setup with Huggingface\u0026rsquo;s script in documentation Here is the code that I tweaked a little bit for my use case. You can find it in my github repo: hf_serve\nFor the devices, you can always use cpu or cuda for GPU.\nAnd if you are working on Mac OS with Apple Silicon, you can use mps to accelerate the inference. More about PyTorch with MPS: PyTorch with MPS. You will need to install the experimental version of PyTorch with MPS support.\nWith MPS, you can get a 2x to 3x speedup on inference.\nIn the repo, the default model used is cardiffnlp/tweet-topic-21-multi .\nYou can change it in the hf_serve/config.py file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 # hf_serve/serving_starlette.py from starlette.applications import Starlette from starlette.responses import JSONResponse from starlette.routing import Route from transformers import pipeline import asyncio from hf_serve.config import DEVICE, MODEL async def homepage(request): payload = await request.body() string = payload.decode(\u0026#34;utf-8\u0026#34;) response_q = asyncio.Queue() await request.app.model_queue.put((string, response_q)) output = await response_q.get() return JSONResponse(output) async def server_loop(q): pipe = pipeline(model=MODEL, top_k=None, device=DEVICE) while True: (string, response_q) = await q.get() out = pipe(string) await response_q.put(out) app = Starlette( routes=[ Route(\u0026#34;/\u0026#34;, homepage, methods=[\u0026#34;POST\u0026#34;]), ], ) @app.on_event(\u0026#34;startup\u0026#34;) async def startup_event(): q = asyncio.Queue() app.model_queue = q asyncio.create_task(server_loop(q)) Then you can execute it with\n1 uvicorn hf_serve.serving_fastapi:app --host 0.0.0.0 --port 8000 To test it, you can use curl or postman to send a POST request to the server.\n1 curl -X POST -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;text\u0026#34;: \u0026#34;Hello world\u0026#34;}\u0026#39; http://localhost:8000 You will see something like this:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 [ [ { \u0026#34;label\u0026#34;: \u0026#34;diaries_\u0026amp;_daily_life\u0026#34;, \u0026#34;score\u0026#34;: 0.6784588694572449 }, { \u0026#34;label\u0026#34;: \u0026#34;other_hobbies\u0026#34;, \u0026#34;score\u0026#34;: 0.16906404495239258 }, { \u0026#34;label\u0026#34;: \u0026#34;relationships\u0026#34;, \u0026#34;score\u0026#34;: 0.1307423859834671 }, { \u0026#34;label\u0026#34;: \u0026#34;film_tv_\u0026amp;_video\u0026#34;, \u0026#34;score\u0026#34;: 0.045063380151987076 }, { \u0026#34;label\u0026#34;: \u0026#34;arts_\u0026amp;_culture\u0026#34;, \u0026#34;score\u0026#34;: 0.04253656417131424 }, { \u0026#34;label\u0026#34;: \u0026#34;celebrity_\u0026amp;_pop_culture\u0026#34;, \u0026#34;score\u0026#34;: 0.02954648993909359 }, { \u0026#34;label\u0026#34;: \u0026#34;music\u0026#34;, \u0026#34;score\u0026#34;: 0.017763793468475342 }, { \u0026#34;label\u0026#34;: \u0026#34;family\u0026#34;, \u0026#34;score\u0026#34;: 0.013196156360208988 }, { \u0026#34;label\u0026#34;: \u0026#34;news_\u0026amp;_social_concern\u0026#34;, \u0026#34;score\u0026#34;: 0.009492534212768078 }, { \u0026#34;label\u0026#34;: \u0026#34;learning_\u0026amp;_educational\u0026#34;, \u0026#34;score\u0026#34;: 0.007527184206992388 }, { \u0026#34;label\u0026#34;: \u0026#34;gaming\u0026#34;, \u0026#34;score\u0026#34;: 0.005994295235723257 }, { \u0026#34;label\u0026#34;: \u0026#34;travel_\u0026amp;_adventure\u0026#34;, \u0026#34;score\u0026#34;: 0.005782891530543566 }, { \u0026#34;label\u0026#34;: \u0026#34;sports\u0026#34;, \u0026#34;score\u0026#34;: 0.005135056562721729 }, { \u0026#34;label\u0026#34;: \u0026#34;business_\u0026amp;_entrepreneurs\u0026#34;, \u0026#34;score\u0026#34;: 0.004528014920651913 }, { \u0026#34;label\u0026#34;: \u0026#34;youth_\u0026amp;_student_life\u0026#34;, \u0026#34;score\u0026#34;: 0.004223798401653767 }, { \u0026#34;label\u0026#34;: \u0026#34;science_\u0026amp;_technology\u0026#34;, \u0026#34;score\u0026#34;: 0.0038613160140812397 }, { \u0026#34;label\u0026#34;: \u0026#34;food_\u0026amp;_dining\u0026#34;, \u0026#34;score\u0026#34;: 0.0025304360315203667 }, { \u0026#34;label\u0026#34;: \u0026#34;fashion_\u0026amp;_style\u0026#34;, \u0026#34;score\u0026#34;: 0.0022351129446178675 }, { \u0026#34;label\u0026#34;: \u0026#34;fitness_\u0026amp;_health\u0026#34;, \u0026#34;score\u0026#34;: 0.0014648066135123372 } ] ] In the original documentation, the model prediction only shows the best label. But in my case, I want to see all the labels and their scores. So I changed the code a little bit.\nUse FastAPI as Web Server I also tried to use FastAPI as web server. This is only because I am more familiar with FastAPI and there is no particular taste for me.\nAnd this makes it a good exercice to learn Starlette and FastApi.\nThe code is in the hf_serve/serving_fastapi.py file.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # hf_serve/serving_fastapi.py from fastapi import FastAPI from fastapi import Request, Response from transformers import pipeline import asyncio from hf_serve.config import DEVICE, MODEL app = FastAPI() @app.post(\u0026#34;/\u0026#34;) async def homepage(request: Request, response: Response): payload = await request.body() string = payload.decode(\u0026#34;utf-8\u0026#34;) response_q = asyncio.Queue() await app.model_queue.put((string, response_q)) output = await response_q.get() response.status_code = 200 return output @app.on_event(\u0026#34;startup\u0026#34;) async def startup_event(): q = asyncio.Queue() app.model_queue = q asyncio.create_task(server_loop(q)) async def server_loop(q): pipe = pipeline(model=MODEL, top_k=None, device=DEVICE) while True: (string, response_q) = await q.get() out = pipe(string) await response_q.put(out) The test will be the same as for Starlette.\nDockerize the Web Server To dockerize the web server, I created a Dockerfile.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 # Use the official Python image as the base image FROM nvidia/cuda:11.7.1-cudnn8-runtime-ubuntu22.04 # Set the working directory in the container WORKDIR /app # Copy the requirements file into the container COPY pyproject.toml . COPY poetry.lock . COPY README.md . COPY hf_serve ./hf_serve # Install the necessary packages RUN apt-get update \u0026amp;\u0026amp; \\ apt-get install -y --no-install-recommends \\ libgomp1 \\ python3.10 \\ python3-pip \\ \u0026amp;\u0026amp; \\ rm -rf /var/lib/apt/lists/* RUN ln -s /usr/bin/python3 /usr/bin/python RUN pip install poetry \u0026amp;\u0026amp; poetry config virtualenvs.create false \u0026amp;\u0026amp; poetry install \u0026amp;\u0026amp; rm -rf ~/.cache # Install pytorch with cuda RUN pip install torch --index-url https://download.pytorch.org/whl/cu118 --no-cache-dir # Expose the port that the application will run on EXPOSE 80 # Start the application when the container starts CMD [\u0026#34;python\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;uvicorn\u0026#34;, \u0026#34;hf_serve.serving_fastapi:app\u0026#34;, \u0026#34;--host\u0026#34;, \u0026#34;0.0.0.0\u0026#34;, \u0026#34;--port\u0026#34;, \u0026#34;80\u0026#34;] It takes the nvidia/cuda:11.7.1-cudnn8-runtime-ubuntu22.04 image as the base image.\nThen it installs the necessary packages and pytorch with cuda.\nFinally, it starts the web server.\nTo build the image, you can use the following command.\n1 docker build -t hf_serve . To run the image, you can use the following command.\n1 docker run -p 8000:80 hf_serve If you want to use the GPU, you can use the following command.\n1 docker run --runtime=nvidia -p 8000:80 hf_serve Conclusion \u0026amp; Performance Test In this article, I showed how to use the transformers library to serve a HF Pipeline with Starlette and FastAPI. FastAPI is not always the best option. It depends on the use case. Since it\u0026rsquo;s widely used, it\u0026rsquo;s a good option to learn.\nIf you are curious about the performance, you can check the benchmark.\nIn this benchmark, Starlette is way faster than FastAPI with almost 40% of difference. (Later on, I cheked on the FastAPI\u0026rsquo;s doc and find out that FastAPI is fully compatible with (and based on) Starlette.) It would be also interesting to see how the performance changes with the number of requests.In the next article, I will use Locust to benchmark the performance of the web server with the pipeline. I want to see also how torchserve can improve the performance compared to these two web servers.\nThen let\u0026rsquo;s see how LRU cache/ Redis can improve the performance of server load in a next article.\n","date":"2023-04-15T12:00:06+09:00","permalink":"https://www.haoxian.icu/p/ml-huggingface-pipeline-as-a-service/","title":"[ML] Huggingface Pipeline as a Service"},{"content":"Context I wanted to test DVC, but it would be better to have a S3-compatible server as storage backend for DVC. I chose Minio because I am kind of with it.\nI set up Minio Operator at the very beginning and then found out it’s a little bit overkill for my use case since it can devour a lot of resources. Then I switched to Minio object storage on K8S which is more suitable for Homelab and learning purpose. For production purposes, it is recommended to use the Minio Operator.\nMinio is an object storage server that is compatible with Amazon S3 APIs. It is designed to be simple, lightweight, and easy to set up.\nTo use Minio on K8S, you will need to deploy the Minio server as a pod. This can be done using the Kubernetes deployment object. Once the pod is running, you can access the Minio server using the service object.\nDeployment To deploy the Minio server, you will need to create a Kubernetes deployment object using the Minio Docker image. You can then expose the deployment as a service using a Kubernetes service object. The service object will allow you to access the Minio server using a stable IP address via the previous mentioned MetalLB with LoadBalancer or NodePort.\nCreation of a persistent volume To create a persistent volume with filesystem mode in Kubernetes, you can use the following example YAML file:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 kind: PersistentVolume apiVersion: v1 metadata: name: minio-pv spec: capacity: storage: 20Gi accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Retain storageClassName: local-storage local: path: /mnt/data nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - your-node-name This file will create a persistent volume with a storage capacity of 20 gigabytes and a local storage path of /mnt/data on the node with the name your-node-name. You can adjust the storage capacity and the local storage path as needed. Once you have created the persistent volume YAML file, you can apply it to your Kubernetes cluster using the kubectl apply command:\n1 kubectl apply -f minio-pv.yaml Replace your-pv-file.yaml with the path to your persistent volume YAML file. Once the persistent volume is created, you can use its name in the persistent volume claim YAML file to request storage from the persistent volume.\nCreation of a persistent volume claim 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 kind: PersistentVolumeClaim apiVersion: v1 metadata: name: minio-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 20Gi volumeMode: Filesystem storageClassName: local-storage selector: matchLabels: type: local This file will create a persistent volume claim with a storage request of 20 gigabytes and a storage class of local-storage. The volumeMode field is set to Filesystem to indicate that the persistent volume will be used as a file system. Once you have created the persistent volume claim YAML file, you can apply it to your Kubernetes cluster using the kubectl apply command:\n1 kubectl apply -f minio-pvc.yaml Deploy Minio server 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 apiVersion: apps/v1 kind: Deployment metadata: name: minio-deployment namespace: minio spec: replicas: 1 selector: matchLabels: app: minio template: metadata: labels: app: minio spec: containers: - name: minio image: minio/minio args: - server - /data - --console-address=0.0.0.0:9001 env: - name: MINIO_ACCESS_KEY value: minio - name: MINIO_SECRET_KEY value: minio123 ports: - containerPort: 9000 - containerPort: 9001 volumeMounts: - name: minio-persistent-storage mountPath: /data volumes: - name: minio-persistent-storage persistentVolumeClaim: claimName: minio-pvc --- apiVersion: v1 kind: Service metadata: name: minio-service namespace: minio spec: selector: app: minio ports: - name: console port: 9000 targetPort: 9000 - name: api port: 9001 targetPort: 9001 type: LoadBalancer externalTrafficPolicy: Local Use MC to access resources on Minio server Assuming that you have set up Minio server and created a bucket, you can use the mc command-line tool to connect to the bucket. First, download and install the mc tool from the official website: https://docs.min.io/docs/minio-client-complete-guide.html\nOnce you have installed mc, you can use the following command to configure it to connect to your Minio server:\n1 mc config host add myminio http://\u0026lt;minio-server-ip\u0026gt;:\u0026lt;minio-server-port\u0026gt; \u0026lt;access-key\u0026gt; \u0026lt;secret-key\u0026gt; Replace \u0026lt;minio-server-ip\u0026gt; and \u0026lt;minio-server-port\u0026gt; with the IP address and port number of your Minio server, respectively. Replace \u0026lt;access-key\u0026gt; and \u0026lt;secret-key\u0026gt; with your Minio access key and secret key, respectively.\nAfter you have configured mc, you can use it to interact with your Minio server. For example, you can list the contents of a bucket with the following command:\n1 mc ls myminio/mybucket Replace mybucket with the name of your bucket. You can also copy files to and from your bucket using mc. For example, you can upload a file to your bucket with the following command:\n1 mc cp /path/to/local/file myminio/mybucket/path/to/remote/file Replace /path/to/local/file with the path to the local file that you want to upload. Replace mybucket/path/to/remote/file with the path to the remote file in your bucket.\nFor more information on how to use mc, please refer to the official documentation: https://docs.min.io/docs/minio-client-complete-guide.html\n","date":"2023-03-27T11:00:06+09:00","permalink":"https://www.haoxian.icu/p/k8s-setup-minio-s3-storage-in-k3s/","title":"[K8S] Setup Minio S3 Storage in K3S"},{"content":"LoadBalancer and load balancer - They are not the same thing! The terms LoadBalancer and load balancer are often used interchangeably, but they can have different meanings depending on the context.\nIn general, a load balancer is a device or software that distributes network traffic across multiple servers or nodes in a network. This can improve the performance and availability of the network by ensuring that no single server or node is overloaded with traffic.\nA LoadBalancer, on the other hand, is a specific type of load balancer that is used in Kubernetes environments. In Kubernetes, a LoadBalancer is a service that provides external access to a set of pods in a deployment. It typically uses a cloud provider\u0026rsquo;s load balancing service to distribute traffic across the pods.\nSo, while a load balancer can refer to any device or software that distributes network traffic, a LoadBalancer specifically refers to a service in Kubernetes that provides external access to a set of pods in a deployment.\nLoadBalancer in K3S with MetalLB K3S is a lightweight Kubernetes distribution designed for use in resource-constrained environments. One of the features of Kubernetes that K3S supports is the LoadBalancer service. This allows traffic to be distributed across multiple pods in a deployment, providing high availability and fault tolerance.\nHowever, in order to use the LoadBalancer service in K3S, you need to have a LoadBalancer implementation that is compatible with the K3S environment. One popular option is MetalLB.\nMetalLB is a software LoadBalancer implementation that runs on Kubernetes. It is designed to integrate with Kubernetes services and provide a LoadBalancer implementation that is easy to configure and use. In K3S, MetalLB can be used to provide LoadBalancer services for your applications.\nTo use MetalLB in K3S, you first need to install the MetalLB controller and the MetalLB speaker. This can be done using the kubectl command-line tool. Once installed, you can configure MetalLB to use a particular IP address range for LoadBalancer services. This IP address range needs to be within the same subnet as the IP address of the K3S node that will be hosting the LoadBalancer service.\nOnce MetalLB is configured, you can create a LoadBalancer service in K3S as you would in a standard Kubernetes environment. This will create a LoadBalancer IP address that can be used to access your application. Traffic will be automatically distributed across the pods in your deployment, providing high availability and fault tolerance.\nIn summary, using the LoadBalancer service in K3S with MetalLB provides an easy-to-use solution for distributing traffic across multiple pods in a deployment. With MetalLB, you can quickly and easily set up LoadBalancer services in K3S, providing high availability and fault tolerance for your applications.\nLayer 2 and BGP in MetalLB MetalLB supports two modes of operation: Layer 2 and BGP.\nIn Layer 2 mode, MetalLB is configured to act as a Layer 2 load balancer. This means that it will use ARP to announce the IP address of the LoadBalancer service to the local network. This mode is useful in environments where BGP is not available or is not desired.\nIn BGP mode, MetalLB is configured to use the Border Gateway Protocol (BGP) to announce the IP address of the LoadBalancer service to the network. This mode is useful in environments where BGP is available and provides a more scalable and flexible solution than Layer 2 mode.\nIn both modes, MetalLB can be configured to use a specific IP address range for LoadBalancer services. This allows you to control the IP addresses that are used for LoadBalancer services and ensure that they are compatible with your network configuration.\nTo choose between Layer 2 and BGP mode in MetalLB, you need to consider your specific network environment and requirements. If you have access to BGP and want a more scalable and flexible solution, BGP mode may be the better choice. If BGP is not available or you prefer a simpler solution, Layer 2 mode may be the better choice.\nWhat did MetalLB actually do? MetalLB assigns an IP address to a LoadBalancer service from the address range that you specify. This IP address is then used to access the service. Then it broadcasts the IP address of the LoadBalancer service to the local network using ARP (Yep, just yelling to everyone in the same network).\nThis allows traffic to be correctly routed to the service, providing high availability and fault tolerance for your applications.\nWith MetalLB, instead of using NodePort, or Ingress which onkly works with HTTP/HTTPS traffic, you can use LoadBalancer service to expose your application to the outside world, thus making the effect of cloud-provisioned Load Balancer.\nHow to set up MetalLB in K3S It’s easy to setup MetalLB. Historically we use helm, but now it’s recommended to use Manifest:\nTo install MetalLB using a manifest in K3S, follow these steps:\nUse the official Manifest or create a new file named metallb.yaml and paste the following contents into the file: 1 kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.13.9/config/manifests/metallb-native.yaml Create a IP pool and apply it.\nBasically the addresses will be a list of IPs that are available from your home router/ your DHCP server. 1 2 3 4 5 6 7 8 apiVersion: metallb.io/v1beta1 kind: IPAddressPool metadata: name: first-pool namespace: metallb-system spec: addresses: - 192.168.88.176-192.168.88.180 Apply the manifest using the kubectl apply command:\n1 kubectl apply -f metallb.yaml This will create a new namespace named metallb-system and deploy the MetalLB controller and speaker as Kubernetes objects within that namespace. Once the controller and speaker are running, you can create LoadBalancer services in K3S and MetalLB will automatically assign an IP address from the specified IP address range.\nNote that you may need to modify your network configuration to allow traffic to be routed to the LoadBalancer IP address. This may involve configuring NAT or firewall rules on your network router or gateway.\nAdvertise your IP pool with Layer2 mode Layer 2 advertisement is used in MetalLB to announce the IP address of the LoadBalancer service to the local network using ARP.\nThis is necessary in Layer 2 mode, which is used when BGP is not available or not desired.\nBy announcing the IP address of the LoadBalancer service to the local network, MetalLB ensures that traffic is correctly routed to the service, providing high availability and fault tolerance for your applications. 1 2 3 4 5 6 7 8 apiVersion: metallb.io/v1beta1 kind: L2Advertisement metadata: name: first-pool-advertisement namespace: metallb-system spec: ipAddressPools: - first-pool Now you can create LoadBalancer services in K3S and MetalLB will automatically assign an IP address from the specified IP address range. Let\u0026rsquo;s try our example with guestbook app. If you didn\u0026rsquo;t see the example, you can find it here: Deploy guestbook example in K3D This time we will use the LoadBalancer service type.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 kind: Service apiVersion: v1 metadata: name: guestbook namespace: guestbook labels: app: guestbook spec: ports: - port: 80 targetPort: http-server selector: app: guestbook type: LoadBalancer Then we will see an IP is assigned to our service by MetalLB.\nReferences https://metallb.universe.tf/installation/\nhttps://www.reddit.com/r/homelab/comments/mvjc0f/metallb_and_traefik_for_a_home_kubernetes_cluster/\nhttps://opensource.com/article/20/7/homelab-metallb\n","date":"2023-03-19T11:00:06+09:00","permalink":"https://www.haoxian.icu/p/k8s-use-metallb-as-k3s-load-balancer/","title":"[K8S] Use MetalLB as K3S Load Balancer"},{"content":"Get your hands dirty with K3D Now that we have a running cluster creating by K3D, it\u0026rsquo;s time to make our very first service.\nThe example that I chosen was from https://github.com/nigelpoulton/k8s-sample-apps (A more PC example can be found from https://kubernetes.io/docs/tutorials/stateless-application/guestbook/) This repo proposed several example apps, and this is the simplest one.\nName Description Notable Features Used Complexity Level Guestbook PHP app with Redis Deployment, Service Beginner WordPress WordPress with MySQL Deployment, Persistent Volume with Claim Beginner Cassandra Cloud Native Cassandra Daemon Set, Stateful Set, Replication Controller Intermediate What\u0026rsquo;s in this Guestbook application This is an PHP application which allows register strings. It looks like this The string will then be kept in a mini Redis cluster.\nIn the actual deployment, there are a frontend and a Redis cluster consisted with a leader and a follower.\nBefore deployment Let\u0026rsquo;s prepare for the deployment with a dedicated namespace. I find this a nice practice before deployment a group of depending services.\nThis allows us to manage the ACL, etc much more easily lately.\nTo create a namespace named guestbook, we use\n1 kubectl create namespace guestbook Then switch the context to use this namespace by defaut:\n1 kubectl config set-context --current --namespace guestbook At the next steps, we will be able to work in this namespace with dedicated informations(pods, deployments, services, etc) for this tutorial.\nRedis deployment The Redis leader Deployment To begin with the Redis part, we will create a redis leader by using this file redis-leader-deployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: replicas: 1 selector: matchLabels: app: redis template: metadata: labels: app: redis role: leader tier: backend spec: containers: - name: leader image: \u0026#34;docker.io/redis:6.0.5\u0026#34; resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 Then type in your terminal with kubectl apply -f redis-leader-deployment.yaml to politely ask our K3D cluster to deploy it. Then checkout the pods with kubectl get pods\nThen you will see the pod is creating\n1 2 NAME READY STATUS RESTARTS AGE redis-leader-857d99cc8-8dtr4 0/1 ContainerCreating 0 6s A few moments later, we will see that the pod is ready\n1 2 NAME READY STATUS RESTARTS AGE redis-leader-857d99cc8-8dtr4 1/1 Running 0 15s Service A deployment always comes with a service for access the pods wisely. Create this file to make the service for our redis leader redis-leader-service.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-leader labels: app: redis role: leader tier: backend spec: ports: - port: 6379 targetPort: 6379 selector: app: redis role: leader tier: backend Then you can apply it with kubectl apply -f redis-leader-service.yaml Check the service with kubectl get service\n1 2 3 kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-leader ClusterIP 10.43.18.93 \u0026lt;none\u0026gt; 6379/TCP 6s The redis follower Deployment As the leader but with slightly different config. Let\u0026rsquo;s create redis-follower-deployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: replicas: 2 selector: matchLabels: app: redis template: metadata: labels: app: redis role: follower tier: backend spec: containers: - name: follower image: gcr.io/google_samples/gb-redis-follower:v2 resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 6379 We create 2 replicas for having 2 followers.\nApply the yaml file kubectl apply -f redis-follower-deployment.yaml and check the deployment with kubectl get pods\n1 2 3 4 NAME READY STATUS RESTARTS AGE redis-leader-857d99cc8-8dtr4 1/1 Running 0 93s redis-follower-9dc7f6964-mcgbn 0/1 ContainerCreating 0 12s redis-follower-9dc7f6964-dpc2b 1/1 Running 0 12s Service Now create the service with redis-follower-service.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: redis-follower labels: app: redis role: follower tier: backend spec: ports: # the port that this service should serve on - port: 6379 selector: app: redis role: follower tier: backend Apply and check the service\n1 2 kubectl apply -f redis-follower-service.yaml kubectl get service 1 2 3 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-leader ClusterIP 10.43.18.93 \u0026lt;none\u0026gt; 6379/TCP 89s redis-follower ClusterIP 10.43.210.57 \u0026lt;none\u0026gt; 6379/TCP 29s Frontend Deployment Now let\u0026rsquo;s move to the frontend part.\nThis is the configuration yaml file frontend-deployment.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: apps/v1 kind: Deployment metadata: name: frontend spec: replicas: 3 selector: matchLabels: app: guestbook tier: frontend template: metadata: labels: app: guestbook tier: frontend spec: containers: - name: php-redis image: gcr.io/google_samples/gb-frontend:v5 env: - name: GET_HOSTS_FROM value: \u0026#34;dns\u0026#34; resources: requests: cpu: 100m memory: 100Mi ports: - containerPort: 80 Apply it with kubectl apply -f frontend-deployment.yaml Let\u0026rsquo;s see the pods with kubectl get pods -l app=guestbook -l tier=frontend\n1 2 3 4 NAME READY STATUS RESTARTS AGE frontend-f7d9c57d4-p9dm8 1/1 Running 0 21s frontend-f7d9c57d4-vgk2v 1/1 Running 0 21s frontend-f7d9c57d4-trqkm 1/1 Running 0 21s Since we have defined our namespace, we can alsow omit the -l app=guestbook which are selectors to choose the coresponding pods.\nService Now let\u0026rsquo;s create a service for the frontend with frontend-service.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # SOURCE: https://cloud.google.com/kubernetes-engine/docs/tutorials/guestbook apiVersion: v1 kind: Service metadata: name: frontend labels: app: guestbook tier: frontend spec: # if your cluster supports it, uncomment the following to automatically create # an external load-balanced IP for the frontend service. # type: LoadBalancer #type: LoadBalancer ports: # the port that this service should serve on - port: 80 selector: app: guestbook tier: frontend Apply it as always kubectl apply -f frontend-service.yaml\nAnd check the service created kubectl get services\n1 2 3 4 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE redis-leader ClusterIP 10.43.18.93 \u0026lt;none\u0026gt; 6379/TCP 3m22s redis-follower ClusterIP 10.43.210.57 \u0026lt;none\u0026gt; 6379/TCP 2m22s frontend NodePort 10.43.120.61 \u0026lt;none\u0026gt; 80:32494/TCP 2s Test our application Now we can test our application by using port-forwarding:\n1 kubectl port-forward svc/frontend 8080:80 If you use a remote server like me for K3D, you may want to add the address of the server with port-forwarding:\n1 kubectl port-forward svc/frontend 8080:80 --address 0.0.0.0 # or the ip of your server 1 2 3 Forwarding from 0.0.0.0:8080 -\u0026gt; 80 Handling connection for 8080 Handling connection for 8080 Scale the frontend Scale up The biggest advantage to use K8S is the capability of scaling.\nWe can use kubectl scale deployment frontend --replicas=5 to scale the frontend.\n1 deployment.apps/frontend scaled Then we check the pods to see if we have 5 replicas for our frontend kubectl get pods -l tier=frontend\n1 2 3 4 5 6 NAME READY STATUS RESTARTS AGE frontend-f7d9c57d4-p9dm8 1/1 Running 0 145m frontend-f7d9c57d4-vgk2v 1/1 Running 0 145m frontend-f7d9c57d4-trqkm 1/1 Running 0 145m frontend-f7d9c57d4-rxftf 1/1 Running 0 68s frontend-f7d9c57d4-xhfw9 1/1 Running 0 68s Hooray!\nScale down Now let\u0026rsquo;s give our poor little server a break by scale down the app kubectl scale deployment frontend --replicas=2 and check the pods again kubectl get pods -l tier=frontend\n1 2 3 NAME READY STATUS RESTARTS AGE frontend-f7d9c57d4-p9dm8 1/1 Running 0 146m frontend-f7d9c57d4-vgk2v 1/1 Running 0 146m Now we can see that we have only two pods for frontend left.\nExpose the service With K3D, we have two ways to expose the service: via ingress and via Nodeport. At the following part, I will show you how to expose the service in K3D.\nUsing NodePort The simplest way to expose the service is to use NodePort. If you don\u0026rsquo;t have the port mapping for the K3D Loadbalancer, then you can add a port for it. Suppose that you don\u0026rsquo;t have 8082 which is the port you want for your app. You can add the port mapping with\n1 k3d cluster edit mycluster --port-add 8082:80@loadbalancer Then you can simply change the frontend service with\n1 kubectl edit svc frontend This gives you the config for your service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 apiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: \u0026gt; {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;guestbook\u0026#34;,\u0026#34;tier\u0026#34;:\u0026#34;frontend\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;frontend\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;guestbook\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;ports\u0026#34;:[{\u0026#34;port\u0026#34;:80}],\u0026#34;selector\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;guestbook\u0026#34;,\u0026#34;tier\u0026#34;:\u0026#34;frontend\u0026#34;},\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;}} creationTimestamp: 2023-03-11T21:06:54Z labels: app: guestbook tier: frontend managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:annotations: .: {} f:kubectl.kubernetes.io/last-applied-configuration: {} f:labels: .: {} f:app: {} f:tier: {} f:spec: f:externalTrafficPolicy: {} f:internalTrafficPolicy: {} f:ports: .: {} \u0026#34;k:{\\\u0026#34;port\\\u0026#34;:80,\\\u0026#34;protocol\\\u0026#34;:\\\u0026#34;TCP\\\u0026#34;}\u0026#34;: .: {} f:port: {} f:protocol: {} f:targetPort: {} f:selector: {} f:sessionAffinity: {} manager: kubectl-client-side-apply operation: Update time: 2023-03-11T21:06:54Z - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:spec: f:ports: \u0026#34;k:{\\\u0026#34;port\\\u0026#34;:80,\\\u0026#34;protocol\\\u0026#34;:\\\u0026#34;TCP\\\u0026#34;}\u0026#34;: f:nodePort: {} f:type: {} manager: Go-http-client operation: Update time: 2023-03-11T21:13:29Z name: frontend namespace: guestbook resourceVersion: \u0026#34;2542766\u0026#34; uid: b50c1161-08f1-4b00-b7b1-0df41ad160f9 spec: clusterIP: 10.43.120.61 clusterIPs: - 10.43.120.61 internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - port: 80 protocol: TCP targetPort: 80 selector: app: guestbook tier: frontend sessionAffinity: None type: ClusterIP status: loadBalancer: {} Now you can edit it by changing the spec part to\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 ... spec: clusterIP: 10.43.120.61 clusterIPs: - 10.43.120.61 internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - nodePort: 8082 port: 80 protocol: TCP targetPort: 80 selector: app: guestbook tier: frontend sessionAffinity: None type: NodePort status: loadBalancer: {} You just need to add nodePort: 8082 and change the type from ClusterIP to NodePort\nNow you can try to access the service from the host of K3D with http://host-ip:8082\nUsing Ingress K3D\u0026rsquo;s documentation recommands this way. I was a little bit confused at the beginning to get it work.\nBut it\u0026rsquo;s as easy as the NodePort method.\nFor the service, you just need to keep it to ClusterIP type and give it a port mapping\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 apiVersion: v1 kind: Service metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: \u0026gt; {\u0026#34;apiVersion\u0026#34;:\u0026#34;v1\u0026#34;,\u0026#34;kind\u0026#34;:\u0026#34;Service\u0026#34;,\u0026#34;metadata\u0026#34;:{\u0026#34;annotations\u0026#34;:{},\u0026#34;labels\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;guestbook\u0026#34;,\u0026#34;tier\u0026#34;:\u0026#34;frontend\u0026#34;},\u0026#34;name\u0026#34;:\u0026#34;frontend\u0026#34;,\u0026#34;namespace\u0026#34;:\u0026#34;guestbook\u0026#34;},\u0026#34;spec\u0026#34;:{\u0026#34;ports\u0026#34;:[{\u0026#34;port\u0026#34;:80}],\u0026#34;selector\u0026#34;:{\u0026#34;app\u0026#34;:\u0026#34;guestbook\u0026#34;,\u0026#34;tier\u0026#34;:\u0026#34;frontend\u0026#34;},\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;}} creationTimestamp: 2023-03-11T21:06:54Z labels: app: guestbook tier: frontend managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:annotations: .: {} f:kubectl.kubernetes.io/last-applied-configuration: {} f:labels: .: {} f:app: {} f:tier: {} f:spec: f:externalTrafficPolicy: {} f:internalTrafficPolicy: {} f:ports: .: {} \u0026#34;k:{\\\u0026#34;port\\\u0026#34;:80,\\\u0026#34;protocol\\\u0026#34;:\\\u0026#34;TCP\\\u0026#34;}\u0026#34;: .: {} f:port: {} f:protocol: {} f:targetPort: {} f:selector: {} f:sessionAffinity: {} manager: kubectl-client-side-apply operation: Update time: 2023-03-11T21:06:54Z - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:spec: f:ports: \u0026#34;k:{\\\u0026#34;port\\\u0026#34;:80,\\\u0026#34;protocol\\\u0026#34;:\\\u0026#34;TCP\\\u0026#34;}\u0026#34;: f:nodePort: {} f:type: {} manager: Go-http-client operation: Update time: 2023-03-11T21:13:29Z name: frontend namespace: guestbook resourceVersion: \u0026#34;2542766\u0026#34; uid: b50c1161-08f1-4b00-b7b1-0df41ad160f9 spec: clusterIP: 10.43.120.61 clusterIPs: - 10.43.120.61 internalTrafficPolicy: Cluster ipFamilies: - IPv4 ipFamilyPolicy: SingleStack ports: - port: 80 protocol: TCP targetPort: 80 selector: app: guestbook tier: frontend sessionAffinity: None type: ClusterIP status: loadBalancer: {} Then you can create an ingress to access the service with guestbook-ingress.yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # apiVersion: networking.k8s.io/v1beta1 # for k3s \u0026lt; v1.19 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: nginx annotations: ingress.kubernetes.io/ssl-redirect: \u0026#34;false\u0026#34; spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: guestbook port: number: 8082 The most important part is to specify the app to guestbook. This is the app selector we defined for the frontend service.\n1 2 3 4 5 backend: service: name: guestbook port: number: 8082 The port number here should be the external port to access the service, and we just tell the LoadBalancer to find the service named guestbook.\nThen just type kubectl apply -f guestbook-ingress.yaml and you are fine to go with your deployment.\nClean up The simplest way to clean up is with the namespace\n1 kubectl delete namespace guestbook A more complicated way it\u0026rsquo;s to purge the pods and the services\n1 2 3 4 kubectl delete deployment -l app=redis kubectl delete service -l app=redis kubectl delete deployment frontend kubectl delete service frontend And something like\n1 2 3 4 deployment.apps \u0026#34;redis-follower\u0026#34; deleted deployment.apps \u0026#34;redis-leader\u0026#34; deleted deployment.apps \u0026#34;frontend\u0026#34; deleted service \u0026#34;frontend\u0026#34; deleted will show up.\nIf you type kubectl get pods, you can see that you cleansed everything.\nReferences https://kubernetes.io/docs/tutorials/stateless-application/guestbook/ https://k3d.io/v5.4.6/usage/exposing_services/\n","date":"2023-03-11T11:00:06+09:00","permalink":"https://www.haoxian.icu/p/k8s-use-k3d-to-implement-the-first-service/","title":"[K8S] Use K3D to implement the first service"},{"content":"Context Recently, I am trying to make full use of my i7-12700 CPU and the 96G RAM that I install for it. The ideal thing for me is to be able to practice MLOps on a k8s cluster. However, it\u0026rsquo;s never easy for bare-metal devices to make a whole cluster with many nodes. Maybe with Docker Desktop it will be easy, but then I may lose the chance to learn.\nI have tested microk8s before. However, there are always some weird things that stop me from just getting the basic control plane. This is probably because it was installed with snap. Anyways, I ran into different solutions. Another solution that I have tested is Minikube. Nice and clean but for a single node. I would like to make it more complicated. K3S is a good one and I have installed it on three low-end laptops with my NAS to make a 1 server and 3 agents cluster. It was funny and I succeeded to run some applications on it. Now that I am on a single machine and I want to simulate a multi-node cluster, the ideal way would be to create several VMs and set up the cluster with them which is tedious. To avoid this, some automation tools like Ansible are one of the best choices. I am not yet ready to get my hands dirty on VMs while my objective is to learn to make things on K8S. K3D is a good alternative based on K3S.\nK3D is a docker version of K3S. It uses docker to simulate nodes and Docker in Docker to run apps on it.\nQuick start with K3D The installation is as handy as the docker installation script:\n1 wget -q -O - https://raw.githubusercontent.com/k3d-io/k3d/main/install.sh | bash To create a cluster:\n1 k3d cluster create test-cluster 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 INFO[0000] Prep: Network INFO[0000] Created network \u0026#39;k3d-test-cluster\u0026#39; INFO[0000] Created image volume k3d-test-cluster-images INFO[0000] Starting new tools node... INFO[0000] Starting Node \u0026#39;k3d-test-cluster-tools\u0026#39; INFO[0001] Creating node \u0026#39;k3d-test-cluster-server-0\u0026#39; INFO[0001] Creating LoadBalancer \u0026#39;k3d-test-cluster-serverlb\u0026#39; INFO[0001] Using the k3d-tools node to gather environment information INFO[0001] HostIP: using network gateway 192.168.144.1 address INFO[0001] Starting cluster \u0026#39;test-cluster\u0026#39; INFO[0001] Starting servers... INFO[0001] Starting Node \u0026#39;k3d-test-cluster-server-0\u0026#39; INFO[0005] All agents already running. INFO[0005] Starting helpers... INFO[0005] Starting Node \u0026#39;k3d-test-cluster-serverlb\u0026#39; INFO[0011] Injecting records for hostAliases (incl. host.k3d.internal) and for 2 network members into CoreDNS configmap... INFO[0013] Cluster \u0026#39;test-cluster\u0026#39; created successfully! INFO[0013] You can now use it like this: kubectl cluster-info A single node cluster is created. Then check the nodes:\n1 kubectl get nodes 1 2 NAME STATUS ROLES AGE VERSION k3d-test-cluster-server-0 Ready control-plane,master 59s v1.25.6+k3s1 Check the cluster status\n1 kubectl cluster-info 1 2 3 4 5 Kubernetes control plane is running at https://0.0.0.0:38483 CoreDNS is running at https://0.0.0.0:38483/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy Metrics-server is running at https://0.0.0.0:38483/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. Create cluster with YAML config It\u0026rsquo;s also a good idea to use YAML file to create the cluster. This allows us to duplicate the cluster easily and easier for us to recreate the whole environment from disaster. For experimental purposes for Devtron, I will make a 3 nodes cluster with 1 server and 2 agents. The convenient part of K3D is that you can spin up more nodes later quickly.\nThe config file is like below:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # devtron_cluster.yaml apiVersion: k3d.io/v1alpha4 kind: Simple metadata: name: cluster servers: 1 agents: 2 ports: - port: 8082:30000 nodeFilters: - loadbalancer - port: 30080-30100:30080-30100 nodeFilters: - loadbalancer Then use:\n1 k3d cluster create --config devtron_cluster.yaml to create the cluster. The important part is load blancer with forwarded ports.\nAs we know we need a public IP for our cluster. Cloud Providers allow you to use their \u0026ldquo;Load Balancer\u0026rdquo; to get a public IP and use it to access the apps inside. However, for on-premise implementation, we need to use solutions like MetaLB. In K3D, we use a container to mimic this by exposing its ports. We will use 8082 for Devtron console and 30080-30100 for our applications.\nA complete configuration file is like below\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 # k3d configuration file, saved as e.g. /home/me/myk3dcluster.yaml apiVersion: k3d.io/v1alpha4 # this will change in the future as we make everything more stable kind: Simple # internally, we also have a Cluster config, which is not yet available externally metadata: name: mycluster # name that you want to give to your cluster (will still be prefixed with `k3d-`) servers: 1 # same as `--servers 1` agents: 2 # same as `--agents 2` kubeAPI: # same as `--api-port myhost.my.domain:6445` (where the name would resolve to 127.0.0.1) host: \u0026#34;myhost.my.domain\u0026#34; # important for the `server` setting in the kubeconfig hostIP: \u0026#34;127.0.0.1\u0026#34; # where the Kubernetes API will be listening on hostPort: \u0026#34;6445\u0026#34; # where the Kubernetes API listening port will be mapped to on your host system image: rancher/k3s:v1.20.4-k3s1 # same as `--image rancher/k3s:v1.20.4-k3s1` network: my-custom-net # same as `--network my-custom-net` subnet: \u0026#34;172.28.0.0/16\u0026#34; # same as `--subnet 172.28.0.0/16` token: superSecretToken # same as `--token superSecretToken` volumes: # repeatable flags are represented as YAML lists - volume: /my/host/path:/path/in/node # same as `--volume \u0026#39;/my/host/path:/path/in/node@server:0;agent:*\u0026#39;` nodeFilters: - server:0 - agent:* ports: - port: 8080:80 # same as `--port \u0026#39;8080:80@loadbalancer\u0026#39;` nodeFilters: - loadbalancer env: - envVar: bar=baz # same as `--env \u0026#39;bar=baz@server:0\u0026#39;` nodeFilters: - server:0 registries: # define how registries should be created or used create: # creates a default registry to be used with the cluster; same as `--registry-create registry.localhost` name: registry.localhost host: \u0026#34;0.0.0.0\u0026#34; hostPort: \u0026#34;5000\u0026#34; proxy: # omit this to have a \u0026#34;normal\u0026#34; registry, set this to create a registry proxy (pull-through cache) remoteURL: https://registry-1.docker.io # mirror the DockerHub registry username: \u0026#34;\u0026#34; # unauthenticated password: \u0026#34;\u0026#34; # unauthenticated volumes: - /some/path:/var/lib/registry # persist registry data locally use: - k3d-myotherregistry:5000 # some other k3d-managed registry; same as `--registry-use \u0026#39;k3d-myotherregistry:5000\u0026#39;` config: | # define contents of the `registries.yaml` file (or reference a file); same as `--registry-config /path/to/config.yaml` mirrors: \u0026#34;my.company.registry\u0026#34;: endpoint: - http://my.company.registry:5000 hostAliases: # /etc/hosts style entries to be injected into /etc/hosts in the node containers and in the NodeHosts section in CoreDNS - ip: 1.2.3.4 hostnames: - my.host.local - that.other.local - ip: 1.1.1.1 hostnames: - cloud.flare.dns options: k3d: # k3d runtime settings wait: true # wait for cluster to be usable before returining; same as `--wait` (default: true) timeout: \u0026#34;60s\u0026#34; # wait timeout before aborting; same as `--timeout 60s` disableLoadbalancer: false # same as `--no-lb` disableImageVolume: false # same as `--no-image-volume` disableRollback: false # same as `--no-Rollback` loadbalancer: configOverrides: - settings.workerConnections=2048 k3s: # options passed on to K3s itself extraArgs: # additional arguments passed to the `k3s server|agent` command; same as `--k3s-arg` - arg: --tls-san=my.host.domain nodeFilters: - server:* nodeLabels: - label: foo=bar # same as `--k3s-node-label \u0026#39;foo=bar@agent:1\u0026#39;` -\u0026gt; this results in a Kubernetes node label nodeFilters: - agent:1 kubeconfig: updateDefaultKubeconfig: true # add new cluster to your default Kubeconfig; same as `--kubeconfig-update-default` (default: true) switchCurrentContext: true # also set current-context to the new cluster\u0026#39;s context; same as `--kubeconfig-switch-context` (default: true) runtime: # runtime (docker) specific options gpuRequest: all # same as `--gpus all` labels: - label: bar=baz # same as `--runtime-label \u0026#39;bar=baz@agent:1\u0026#39;` -\u0026gt; this results in a runtime (docker) container label nodeFilters: - agent:1 Now you have a running K3D cluster on your machine. In the next post, I will show you how to install devtron for CI/CD and so on.\nReferences https://k3d.io/v5.4.7/\nNotes I finally moved to K3S because I got more machines and I wanted to have a real cluster.\n","date":"2023-02-18T13:00:06+09:00","permalink":"https://www.haoxian.icu/p/k8s-setup-k3d/","title":"[K8S] Setup K3D"}]