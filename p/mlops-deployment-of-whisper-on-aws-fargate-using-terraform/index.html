<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="A quick guide to deploy Whisper on AWS Fargate using Terraform"><title>[MLOps] Deployment of Whisper on AWS Fargate using Terraform</title><link rel=canonical href=https://www.haoxian.icu/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/><link rel=stylesheet href=/scss/style.min.833d6eed45de56f48306bf57268d5b8cdfc8a60e8e7bdc99810464fcd033f7c6.css><meta property='og:title' content="[MLOps] Deployment of Whisper on AWS Fargate using Terraform"><meta property='og:description' content="A quick guide to deploy Whisper on AWS Fargate using Terraform"><meta property='og:url' content='https://www.haoxian.icu/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/'><meta property='og:site_name' content="Haoxian's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='Whisper'><meta property='article:tag' content='AWS'><meta property='article:tag' content='Fargate'><meta property='article:tag' content='Terraform'><meta property='article:tag' content='MLOps'><meta property='article:tag' content='Docker'><meta property='article:published_time' content='2024-02-01T11:00:06+09:00'><meta property='article:modified_time' content='2024-02-01T11:00:06+09:00'><meta name=twitter:title content="[MLOps] Deployment of Whisper on AWS Fargate using Terraform"><meta name=twitter:description content="A quick guide to deploy Whisper on AWS Fargate using Terraform"><link rel="shortcut icon" href=/favicon.svg></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_a8dd701257cc3205.jpg width=300 height=400 class=site-logo loading=lazy alt=Avatar></a></figure><div class=site-meta><h1 class=site-name><a href=/>Haoxian's Blog</a></h1><h2 class=site-description>From NLP to MLOps and LLM</h2></div></header><ol class=menu-social><li><a href=https://github.com/wang-haoxian target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#introduction>Introduction</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#interpretation-of-the-task>Interpretation of the task</a><ol><li><a href=#reference-of-solutions-on-the-market>Reference of solutions on the market</a></li></ol></li><li><a href=#investigation-on-whisper-and-its-deployment>Investigation on Whisper and its deployment</a><ol><li><a href=#frameworks-and-libraries>Frameworks and Libraries</a></li><li><a href=#large-v2-model-on-gpu>Large-v2 model on GPU</a></li><li><a href=#small-model-on-cpu>Small model on CPU</a></li><li><a href=#possible-solutions-on-aws>Possible Solutions on AWS</a></li></ol></li><li><a href=#overview-of-the-solution>Overview of the solution</a></li><li><a href=#python-implementation>Python Implementation</a><ol><li><a href=#development-details>Development details</a></li></ol></li><li><a href=#terraform-implementation>Terraform Implementation</a><ol><li><a href=#terraform-cloud>Terraform cloud</a></li><li><a href=#development-details-1>Development details</a><ol><li><a href=#boostrap>Boostrap</a></li><li><a href=#app-deployment>App Deployment</a></li><li><a href=#resources-estimation>Resources Estimation</a></li></ol></li></ol></li><li><a href=#thoughts-on-potential-improvements>Thoughts on potential improvements</a><ol><li><a href=#security>Security</a></li><li><a href=#scalability>Scalability</a></li><li><a href=#efficiency>Efficiency</a></li><li><a href=#functionality>Functionality</a></li><li><a href=#viability>Viability</a></li><li><a href=#housekeeping>Housekeeping</a></li><li><a href=#cost-agnostic>Cost Agnostic</a></li><li><a href=#fallback>Fallback</a></li></ol></li><li><a href=#in-the-end>In the end</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><div class=article-title-wrapper><h2 class=article-title><a href=/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/>[MLOps] Deployment of Whisper on AWS Fargate using Terraform</a></h2><h3 class=article-subtitle>A quick guide to deploy Whisper on AWS Fargate using Terraform</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2024-02-01T11:00:06+09:00>Feb 01, 2024</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>10 minute read</time></div></footer></div></header><section class=article-content><h2 id=introduction>Introduction</h2><p>Whisper is a STT (Speech to Text) model developed by <a class=link href=https://openai.com/research/whisper target=_blank rel=noopener>OPENAI</a>. It&rsquo;s a powerful model that can convert human speech into text. Friend of mine encoutered this project as an job interview task with IaC using Terraform so I get the idea to do it on my own and I find it interesting to deploy it on AWS Fargate. I chose Fargate because of the highly optimized version of it doesn&rsquo;t require GPUs. In this post, I will share my journey to this final solution and show you how to deploy it.<br>All the code is available on <a class=link href=https://github.com/wang-haoxian/whisper-terraform-aws-Fargate target=_blank rel=noopener>Github</a> and you can use it to deploy the model on your own AWS account.</p><h2 id=prerequisites>Prerequisites</h2><ul><li>AWS CLI configured</li><li>Terraform with Terraform Cloud (or local state if you prefer) configured</li><li>Docker installed</li></ul><p>If you don&rsquo;t have any experience on Terraform, you can use the official tutorial to get started: <a class=link href="https://learn.hashicorp.com/tutorials/terraform/install-cli?in=terraform/aws-get-started" target=_blank rel=noopener>Getting Started with Terraform</a> with AWS Provider.</p><h2 id=interpretation-of-the-task>Interpretation of the task</h2><p>Due to the confidentiality of the task, I can&rsquo;t share the original task. However, I can share the main points of the task. The task is to serve STT model to replace the previous outsourced service. The model is expected to serve 100 users in a call center for post-call analytics and we should expect X10 users in the future.<br>My understanding on this task is:</p><ul><li>This is not about the concurrency of the model. We don&rsquo;t need real-time processing, so we can use batch processing.</li><li>The number of users is not a significant factor for the service, but the number of audios generated and the processing time for each document. (For example, we want to have the transcriptions of all our last weeks calls to have insights in the weekly meetings this monday, that means the time window to process all the files before monday, so for the files of Friday we will have to process them during the weekend, without taking consideration of the time that analytics team should take).</li><li>The model is not expected to be used by the public, so we don&rsquo;t need to worry about the security of the model.(At least for this version, we can assume that we only use the model in a secure environment)</li><li>The cost should be minimized. As long as we can optimize the model to use the least resources, we can use the cheapest service instead of the most powerful one with GPUs.</li><li>The model should be scalable. We should be able to scale the model to serve more users in the future.</li><li>We can create S3 buckets to store the audio files and the results, but we don&rsquo;t have to since the end user of the service(the analytics Team) may have already had a solution for this.</li><li>There are no limit on the choice of the model. We can use any model as long as it can serve the purpose. But we should be able to evaluate the model with some baseline metrics.(e.g. WER, CER, etc. But this is not the main point of the task)</li><li>The Diarisation is not mentionned but important in the multi-party communications like telephones, but that should be another project since we are talking about another ML module.</li></ul><h3 id=reference-of-solutions-on-the-market>Reference of solutions on the market</h3><p>In France, I have encountered a few companies that provide STT services. I think it&rsquo;s interesting to check out <a class=link href=https://www.allo-media.net/ target=_blank rel=noopener>AlloMedia</a>, a company that provides STT services for call centers. We can be inspired by their solutions.</p><h2 id=investigation-on-whisper-and-its-deployment>Investigation on Whisper and its deployment</h2><h3 id=frameworks-and-libraries>Frameworks and Libraries</h3><p>There are several possible frameworks to use Whisper model, mainly:</p><ul><li><a class=link href=https://github.com/openai/whisper target=_blank rel=noopener>OpenAI official</a> The official OpenAI implementation</li><li><a class=link href=https://github.com/ggerganov/whisper.cpp target=_blank rel=noopener>Whisper.CPP</a> The C++ implementation of the model</li><li><a class=link href=https://huggingface.co/openai/whisper-large-v3 target=_blank rel=noopener>Transformers</a> The Huggingface implementation of the model in its transformers library.</li><li><a class=link href=https://github.com/systran/faster-whisper target=_blank rel=noopener>Faster Whisper</a> which converts the model to <a class=link href=https://github.com/OpenNMT/CTranslate2 target=_blank rel=noopener>CTranslate2</a> format to optimize the model.</li></ul><p>There is a benchmark of these implementations on <a class=link href=https://github.com/SYSTRAN/faster-whisper/blob/master/README.md target=_blank rel=noopener>this page</a> and I take some of the information here:</p><h3 id=large-v2-model-on-gpu>Large-v2 model on GPU</h3><div class=table-wrapper><table><thead><tr><th>Implementation</th><th>Precision</th><th>Beam size</th><th>Time</th><th>Max. GPU memory</th><th>Max. CPU memory</th></tr></thead><tbody><tr><td>openai/whisper</td><td>fp16</td><td>5</td><td>4m30s</td><td>11325MB</td><td>9439MB</td></tr><tr><td>faster-whisper</td><td>fp16</td><td>5</td><td>54s</td><td>4755MB</td><td>3244MB</td></tr><tr><td>faster-whisper</td><td>int8</td><td>5</td><td>59s</td><td>3091MB</td><td>3117MB</td></tr></tbody></table></div><p><em>Executed with CUDA 11.7.1 on a NVIDIA Tesla V100S.</em></p><h3 id=small-model-on-cpu>Small model on CPU</h3><div class=table-wrapper><table><thead><tr><th>Implementation</th><th>Precision</th><th>Beam size</th><th>Time</th><th>Max. memory</th></tr></thead><tbody><tr><td>openai/whisper</td><td>fp32</td><td>5</td><td>10m31s</td><td>3101MB</td></tr><tr><td>whisper.cpp</td><td>fp32</td><td>5</td><td>17m42s</td><td>1581MB</td></tr><tr><td>whisper.cpp</td><td>fp16</td><td>5</td><td>12m39s</td><td>873MB</td></tr><tr><td>faster-whisper</td><td>fp32</td><td>5</td><td>2m44s</td><td>1675MB</td></tr><tr><td>faster-whisper</td><td>int8</td><td>5</td><td>2m04s</td><td>995MB</td></tr></tbody></table></div><p><em>Executed with 8 threads on a Intel(R) Xeon(R) Gold 6226R.</em></p><p>As we can see, the faster-whisper implementation is the most optimized one. It&rsquo;s interesting to use this implementation for our deployment.</p><p>This is why I chose to use CPU and Fargate. After some tests, I found that the base model is very optimized and can be used on CPU and we can get the result in a reasonable time without the cost of quality.</p><p>The code to serve the model is very simple, we can use the following code to serve the model:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>faster_whisper</span> <span class=kn>import</span> <span class=n>WhisperModel</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_size</span> <span class=o>=</span> <span class=s2>&#34;large-v3&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Run on GPU with FP16</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>WhisperModel</span><span class=p>(</span><span class=n>model_size</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span> <span class=n>compute_type</span><span class=o>=</span><span class=s2>&#34;float16&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># or run on GPU with INT8</span>
</span></span><span class=line><span class=cl><span class=c1># model = WhisperModel(model_size, device=&#34;cuda&#34;, compute_type=&#34;int8_float16&#34;)</span>
</span></span><span class=line><span class=cl><span class=c1># or run on CPU with INT8</span>
</span></span><span class=line><span class=cl><span class=c1># model = WhisperModel(model_size, device=&#34;cpu&#34;, compute_type=&#34;int8&#34;)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>segments</span><span class=p>,</span> <span class=n>info</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>transcribe</span><span class=p>(</span><span class=s2>&#34;audio.mp3&#34;</span><span class=p>,</span> <span class=n>beam_size</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=s2>&#34;Detected language &#39;</span><span class=si>%s</span><span class=s2>&#39; with probability </span><span class=si>%f</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=p>(</span><span class=n>info</span><span class=o>.</span><span class=n>language</span><span class=p>,</span> <span class=n>info</span><span class=o>.</span><span class=n>language_probability</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>segment</span> <span class=ow>in</span> <span class=n>segments</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=s2>&#34;[</span><span class=si>%.2f</span><span class=s2>s -&gt; </span><span class=si>%.2f</span><span class=s2>s] </span><span class=si>%s</span><span class=s2>&#34;</span> <span class=o>%</span> <span class=p>(</span><span class=n>segment</span><span class=o>.</span><span class=n>start</span><span class=p>,</span> <span class=n>segment</span><span class=o>.</span><span class=n>end</span><span class=p>,</span> <span class=n>segment</span><span class=o>.</span><span class=n>text</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>As you can see, the code is very simple and we can use it to serve the model.</p><h3 id=possible-solutions-on-aws>Possible Solutions on AWS</h3><p>There are several possible way to deploy the model on AWS, mainly:</p><ul><li>EC2: We can deploy the model on EC2 and use the autoscaling group to scale the model.</li><li>Lambda: We can deploy the model on Lambda and use the API Gateway to serve the model.</li><li>Fargate: We can deploy the model on Fargate and use the ECS to serve the model.</li><li>SageMaker: We can deploy the model on SageMaker and use the endpoint to serve the model.</li><li>EKS: We can deploy the model on EKS and use the Kubernetes to serve the model.</li></ul><p>I chose to use Fargate because it&rsquo;s the most optimized solution for our case. We don&rsquo;t need GPUs and we don&rsquo;t need to worry about the infrastructure. We can use the Fargate to serve the model and we can use the ECS to scale the model.</p><p>Let&rsquo;s still checkout the pros and cons of each solution:</p><div class=table-wrapper><table><thead><tr><th>Solution</th><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>EC2</td><td>Full control of the infrastructure</td><td>Need to manage the infrastructure and expensive when using GPU</td></tr><tr><td>Lambda</td><td>Serverless</td><td>Limited to 15 minutes and no GPU</td></tr><tr><td>Fargate</td><td>Serverless</td><td>No GPU</td></tr><tr><td>SageMaker</td><td>Managed service</td><td>Expensive</td></tr><tr><td>EKS</td><td>Full control of the infrastructure</td><td>Need to manage the infrastructure</td></tr></tbody></table></div><h2 id=overview-of-the-solution>Overview of the solution</h2><p>I draw the pipeline of the deployment as follows:
<img src=/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/whisper-arch-base.png width=2243 height=1549 srcset="/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/whisper-arch-base_hu_fcb5d54f06878abc.png 480w, /p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/whisper-arch-base_hu_a01300e56a2cc122.png 1024w" loading=lazy alt="The illustrated pipeline" class=gallery-image data-flex-grow=144 data-flex-basis=347px></p><p>We can expect three parts in the deployment:</p><ul><li>Codes<ul><li>The Python code for model serving code: The code to serve the model.</li><li>The Terraform code for infrastructure: The code to deploy the infrastructure.</li></ul></li><li>The infrastructure part: The infrastructure of code and CI/CD pipeline. This part is rather static and normally in the organization, it&rsquo;s managed by the DevOps team or the Infra team. And it&rsquo;s not the main part of the task and not necessary to be done with AWS. It can be Github Actions, Gitlab CI, Jenkins, etc.</li><li>The model serving part: The model is served by the Fargate and the ECS. We can use the ECS to scale the model.</li></ul><h2 id=python-implementation>Python Implementation</h2><h3 id=development-details>Development details</h3><ul><li><p>Python Project management: Poetry</p></li><li><p>Model Framework: CTranslate2 for faster-whisper</p></li><li><p>Model size: Base</p></li><li><p>API Framework: FastAPI with Swagger UI and OpenAPI support</p></li><li><p>Model is downloaded when the web server is loaded (Not the best practice)</p></li><li><p>An endpoint <code>/transcribe</code> for transcription</p></li><li><p>A health check <code>/healthz</code> is exposed for healthcheck ( for the sake of time, no model status check is implemented in this version) it’s just a hook to see if the server responds</p></li><li><p>A dockerfile with docker-compose is created for building the image. And to launch it easily without typing long commands for dev purpose.</p></li><li><p>The docker compose file has limited CPU and memory to help with resources provisioning</p></li><li><p>.gitignore and .dockerignore are added for not including unwanted files in docker building process or in git.</p></li><li><p>The repo coexists with terraform codes, in reality they should be either separated or as submodules.</p></li><li><p>Formatting and linting:</p><ul><li>Black</li><li>Pylint</li><li>MyPy</li><li>isort</li></ul></li><li><p>Pydantic is used to make schemes for the API</p></li><li><p>A makefile is created for handy commands for testing</p></li></ul><h2 id=terraform-implementation>Terraform Implementation</h2><p>I have no previous experience on terraform except several runs with the official tutorials. I learnt during the project. To begin with, I took
<a class=link href=https://github.com/kayvane1/terraform-aws-huggingface-deploy/tree/master target=_blank rel=noopener>GitHub - kayvane1/terraform-aws-huggingface-deploy</a>
to get inspired. ChatGPT was used to help me understand quickly the syntax and make sample codes based on my needs, but mostly I use the official documentation.</p><h3 id=terraform-cloud>Terraform cloud</h3><p>In this project I use terraform cloud all the time to simulate an env for collaboration as team (shared secrets, states, etc) It could be S3 backend or whatever backend supported by Terraform</p><h3 id=development-details-1>Development details</h3><h4 id=boostrap>Boostrap</h4><p>The project is modularized to four modules, instead of just a file.</p><p>I consider this part as basic infrastructure that is used around the whole infrastructure. It should be easily tested one by one.</p><p>there are four modules:</p><ol><li>Code Commit</li><li>Code Build</li><li>ECR</li><li>IAM (to be put on corresponding modules instead of being an independent module)</li></ol><h4 id=app-deployment>App Deployment</h4><p>In contrary to the bootstrap part, I think this part is much more exclusive to the app. So I put every components in a single <code>ecs</code> module with alb.tf, <a class=link href=http://main.tf target=_blank rel=noopener>main.tf</a>, etc.</p><h4 id=resources-estimation>Resources Estimation</h4><p>Based on the local runs, I begin with</p><h5 id=take-1>Take 1</h5><ul><li><p>2 CPUs + 4GB</p></li><li><p>In fact only 65 of 2 CPU and 11% of RAM was used</p><p><img src=/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/take1.png width=2074 height=702 srcset="/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/take1_hu_1e61f75ea7752e6b.png 480w, /p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/take1_hu_c5ba67cd7ae18102.png 1024w" loading=lazy alt="CPU and memory consumption of take1" class=gallery-image data-flex-grow=295 data-flex-basis=709px></p></li><li><p>So reduced to 1 CPU + 2GB but it’s not sufficient can caused the service to stop</p></li></ul><h5 id=take2>Take2</h5><ul><li>1 CPU + 2GB</li></ul><p><img src=/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/take2.png width=2134 height=762 srcset="/p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/take2_hu_b9cc39d32d5466c5.png 480w, /p/mlops-deployment-of-whisper-on-aws-fargate-using-terraform/take2_hu_5da2a03c6d3f5373.png 1024w" loading=lazy alt="CPU and memory consumption of take1" class=gallery-image data-flex-grow=280 data-flex-basis=672px></p><ul><li>This cause the container to be killed often</li></ul><h5 id=final-choice>Final choice</h5><ul><li>2 CPU + 4 GB of RAM should be ok but we need further stress test</li></ul><h5 id=alb-strategy>ALB strategy</h5><p>I use ALB&rsquo;s target group for autoscaling the service. The desired number was set to 3 and be able to scale to 0. The minimum health percent of app was set to 50 to trigger the scale in.</p><h2 id=thoughts-on-potential-improvements>Thoughts on potential improvements</h2><h3 id=security>Security</h3><ul><li>https (even for internal communication - zero trust) + dedicated domain name (if exposed to the internet, if it’s called by enduser’s PC, not sure what the users look like) + with IP whitelist to improve security</li><li>dev/stage/prod env separation</li><li>Finer grain RBAC</li></ul><h3 id=scalability>Scalability</h3><ul><li>Automatic Target Weights (ATW) → weighted lb. Combine different strategies with metrics to make the scaling efficiently or Least outstanding requests</li><li>stress test</li><li>Over-Provisioning</li><li>Fine-Tune threshold for scaling based on metrics</li><li>Rate Limit in app</li><li>Queuing</li></ul><h3 id=efficiency>Efficiency</h3><ul><li>Improve configurability with more tf variables instead of hardcoded ones</li><li>Cache for Code Build</li><li>tag commit for code build and tag image (versioning of images), make two pipelines one for dev CI another for publish based on branch/tag system</li><li>region - I picked randomly</li><li>triggers<ul><li>Fargate deploy refresh should be triggered in pipeline once the prod build is done</li><li>Code commit should trigger code pipeline automatically with push/merge to main</li></ul></li><li>systematically use tags</li></ul><h3 id=functionality>Functionality</h3><ul><li>Is Terraform cloud the standard usage of Doctolib? Maybe just with S3 backend</li><li>better log/ monitoring</li><li>EFS for models</li></ul><h3 id=viability>Viability</h3><ul><li>health check container → the health check is oversimplified</li><li>The model is downloaded from HFhub which is an external service, and if it fails, the service fails. So the model file should be persistent to S3 or add to the image.</li><li>P-99 TM-99</li></ul><h3 id=housekeeping>Housekeeping</h3><ul><li>aws_ecr_lifecycle_policy for clean up old images that are not in use</li></ul><h3 id=cost-agnostic>Cost Agnostic</h3><ul><li>cost Optim</li></ul><h3 id=fallback>Fallback</h3><ul><li>Use AWS TTS as a fallback plan</li></ul><h2 id=in-the-end>In the end</h2><p>This is what I have done in this short-term project. I will continue to improve the project and I will be happy to hear your feedback.</p></section><footer class=article-footer><section class=article-tags><a href=/tags/whisper/>Whisper</a>
<a href=/tags/aws/>AWS</a>
<a href=/tags/fargate/>Fargate</a>
<a href=/tags/terraform/>Terraform</a>
<a href=/tags/mlops/>MLOps</a>
<a href=/tags/docker/>Docker</a></section></footer></article><div class=disqus-container><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//hugo-theme-stack.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></div><style>.disqus-container{background-color:var(--card-background);border-radius:var(--card-border-radius);box-shadow:var(--shadow-l1);padding:var(--card-padding)}</style><script>window.addEventListener("onColorSchemeChange",e=>{typeof DISQUS=="object"&&DISQUS.reset({reload:!0})})</script><footer class=site-footer><section class=copyright>&copy;
2023 -
2025 Haoxian's Blog</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.33.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>